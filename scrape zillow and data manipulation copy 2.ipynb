{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Zillow.com to analyze housing price in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal here is to collect housing prices for both rental and sale in New York city. I looked at three major real estate website including Trulia, Zillow, and StreetEasy. Comparing to the other two websites, StreetEasy gives the most information on the searching results page and the format of each listing is very consistent, which is great for the purpose of web-scraping.<br\\ >\n",
    "<a href=\"http://zillow.com/\">\n",
    "<img \"StreetEasy\" src=\"map/streetEasy_logo.jpg\" height=\"30px\" width=\"150px\"/></a><br\\ >\n",
    "\n",
    "Web scraping is done using the beautifulsoup package in Python. I created two functions that can loop through all the pages of searching results, and also empty strings to store results. Below are the steps I took to scrape StreetEasy:\n",
    "1. Analyzing the HTML page: HTML code of a web page can be viewed by right click and selecting 'Inspect'. This helps us identifying the HTML tags of the information to be scraped\n",
    "2. Making the soup!: It is important to select the correct parser for your data type. I used HTML parser.\n",
    "3. Navigating the parse tree and iterate through tags: once the soup is made, we have the HTML code in Python. We can then find our desired information by searching through HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import lxml\n",
    "import numbers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://streeteasy.com/for-sale/nyc/', 'https://streeteasy.com/for-sale/nyc/page_2', 'https://streeteasy.com/for-sale/nyc/page_3', 'https://streeteasy.com/for-sale/nyc/page_4', 'https://streeteasy.com/for-sale/nyc/page_5', 'https://streeteasy.com/for-sale/nyc/page_6', 'https://streeteasy.com/for-sale/nyc/page_7', 'https://streeteasy.com/for-sale/nyc/page_8', 'https://streeteasy.com/for-sale/nyc/page_9', 'https://streeteasy.com/for-sale/nyc/page_10', 'https://streeteasy.com/for-sale/nyc/page_11', 'https://streeteasy.com/for-sale/nyc/page_12', 'https://streeteasy.com/for-sale/nyc/page_13', 'https://streeteasy.com/for-sale/nyc/page_14', 'https://streeteasy.com/for-sale/nyc/page_15', 'https://streeteasy.com/for-sale/nyc/page_16', 'https://streeteasy.com/for-sale/nyc/page_17', 'https://streeteasy.com/for-sale/nyc/page_18', 'https://streeteasy.com/for-sale/nyc/page_19', 'https://streeteasy.com/for-sale/nyc/page_20']\n"
     ]
    }
   ],
   "source": [
    "req_headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-encoding': 'gzip, deflate, br',\n",
    "    'Accept-language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-insecure-requests': '1',\n",
    "    'User-agent': UserAgent().random,\n",
    "}\n",
    "\n",
    "#base_url = \"https://www.zillow.com/homes/for_sale/\"\n",
    "base_url = \"https://streeteasy.com/for-sale/\"\n",
    "urls = []\n",
    "\n",
    "city = 'nyc'\n",
    "url1 = base_url +city+'/'\n",
    "urls.append(url1)\n",
    "\n",
    "start_page = 2\n",
    "end_page = 20\n",
    "\n",
    "# Add all pages\n",
    "for i in range(start_page, end_page + 1):\n",
    "    dom = base_url + city + '/' + 'page_' + str(i) #streeteasy\n",
    "    #dom = base_url + city + '/' + str(i) '_p' +'/' #zillow\n",
    "    if dom not in urls:\n",
    "        urls.append(dom)\n",
    "\n",
    "print(urls)\n",
    "\n",
    "# Define the rate limit: e.g., 5 calls per 60 seconds\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=60)\n",
    "def soups(data):\n",
    "    with requests.Session() as s:\n",
    "        r = s.get(data, headers=req_headers)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        #print(soup.prettify())  # Corrected line: Print prettified HTML content from the soup object\n",
    "    return soup\n",
    "\n",
    "\n",
    "#for url in urls:\n",
    "#    htmls=soups(url)\n",
    "# Call soup function and store output in a list\n",
    "#lst = []\n",
    "\n",
    "#for url in urls:\n",
    "#    htmls = soups(url)\n",
    "\n",
    "#print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time to fetch data from 30 pages: 30.0 minutes\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "rate_limit = 5 #calls/pages per minute\n",
    "period = 60 #a period of 60 seconds\n",
    "\n",
    "def estimate_time(rate_limit, period):\n",
    "    time_needed = pages / rate_limit #pages/5calls/minute\n",
    "    minutes = math.floor(time_needed)\n",
    "    seconds = math.ceil((time_needed - minutes) * 60)\n",
    "    return minutes, seconds\n",
    "\n",
    "# Example usage:\n",
    "pages = 98\n",
    "\n",
    "minutes, seconds = estimate_time(pages, rate_limit)\n",
    "print(f\"Estimated time needed: {minutes} minutes and {seconds} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store listing information\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Iterate through each listing card and extract relevant information\n",
    "            # Extract building type and neighborhood\n",
    "        building_info = card.find(class_=\"listingCardLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        # Extract address\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "\n",
    "        # Extract price\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        # Extract bed, bathroom details, and size\n",
    "        description = card.find('div', class_='description').text\n",
    "        bedrooms = card.find('div', class_='bedrooms').text\n",
    "        bathrooms = card.find('div', class_='bathrooms').text\n",
    "        size = card.find('div', class_='size').text\n",
    "\n",
    "        # Extract amenities match\n",
    "        amenities_list = card.find('ul', class_='amenities').find_all('li')\n",
    "        amenities = [amenity.text for amenity in amenities_list]\n",
    "\n",
    "        # property url\n",
    "        url = card.find('a', class_='listingCard-link jsCardLinkGA featured-link-to-hdp')['href']\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"address\": address,\n",
    "             \"building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"size (sq. ft.)\": size,\n",
    "            \"price\": price,\n",
    "            \"# bedrooms\": bedrooms,\n",
    "            \"# bathrooms\": bathrooms,\n",
    "            \"amenities\": amenities,\n",
    "            \"description\": description,\n",
    "            \"url\": url\n",
    "        \n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "        # Append the listing dictionary to the list of listings\n",
    "        \n",
    "            #address = data.find_all(class_= 'list-card-addr') zillow\n",
    "            #price = list(data.find_all(class_='list-card-price')) zillow\n",
    "            #beds = list(data.find_all(\"ul\", class_=\"list-card-details\")) zillow\n",
    "            #last_updated = data.find_all('div', {'class': 'list-card-top'}) zillow\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage:\n",
    "# Define the URL from which you want to fetch the HTML content\n",
    "#url = \"your_url_here\"\n",
    "\n",
    "# Fetch the HTML content from the URL\n",
    "#soup = soups(url)\n",
    "\n",
    "# Parse the HTML content and extract listing information from JSON-LD data\n",
    "listings = parse_soup(soup)\n",
    "\n",
    "# Create a pandas DataFrame from the extracted listing information\n",
    "df = pd.DataFrame(listings)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitException\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Coding/Web_Scraping_StreetEasty/.venv/lib/python3.11/site-packages/ratelimit/decorators.py:113\u001b[0m, in \u001b[0;36msleep_and_retry.<locals>.wrapper\u001b[0;34m(*args, **kargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitException \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/Coding/Web_Scraping_StreetEasty/.venv/lib/python3.11/site-packages/ratelimit/decorators.py:77\u001b[0m, in \u001b[0;36mRateLimitDecorator.__call__.<locals>.wrapper\u001b[0;34m(*args, **kargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_on_limit:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many calls\u001b[39m\u001b[38;5;124m'\u001b[39m, period_remaining)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRateLimitException\u001b[0m: too many calls",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m all_listings \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# List to store all dataframes\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m---> 72\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43msoups\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     listings \u001b[38;5;241m=\u001b[39m parse_soup(soup)\n\u001b[1;32m     74\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(listings)\n",
      "File \u001b[0;32m~/Coding/Web_Scraping_StreetEasty/.venv/lib/python3.11/site-packages/ratelimit/decorators.py:115\u001b[0m, in \u001b[0;36msleep_and_retry.<locals>.wrapper\u001b[0;34m(*args, **kargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitException \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m--> 115\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(exception\u001b[38;5;241m.\u001b[39mperiod_remaining)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Define a defaultdict to store the occurrence count of each address\n",
    "address_counter = defaultdict(int)\n",
    "\n",
    "def extract_numeric_value(text):\n",
    "    if text is not None:\n",
    "        # Use regular expression to extract numerical values, including optional units\n",
    "        numeric_value = re.search(r'(\\d{1,3}(,\\d{3})*(\\.\\d+)?)\\s*(?:square\\s*feet)?', text)\n",
    "        if numeric_value:\n",
    "            # Remove commas from the extracted value\n",
    "            numeric_value_without_commas = numeric_value.group(1).replace(',', '')\n",
    "            return numeric_value_without_commas  # Return the extracted numeric value\n",
    "    return None\n",
    "\n",
    "# Function to parse the HTML soup\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Extract relevant information from each listing card\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "        \n",
    "        building_info = card.find('p',class_=\"listingCardLabel listingCardLabel-grey listingCard-upperShortLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        bed_elem = card.find('span', class_='listingDetailDefinitionsIcon--bed')\n",
    "        beds_text = bed_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bed_elem else None\n",
    "        beds = extract_numeric_value(beds_text)\n",
    "\n",
    "        bath_elem = card.find('span', class_='listingDetailDefinitionsIcon--bath')\n",
    "        baths_text = bath_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bath_elem else None\n",
    "        baths = extract_numeric_value(baths_text)\n",
    "\n",
    "        # Extract size\n",
    "        size_elem = card.find('span', class_='listingDetailDefinitionsIcon--measure')\n",
    "        size_text = size_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if size_elem else None\n",
    "        size = extract_numeric_value(size_text)\n",
    "\n",
    "        url_element = card.find('a', class_='listingCard-link jsCardLinkGA')\n",
    "        url = url_element.get('href') if url_element else 'Property URL not found.'\n",
    "\n",
    "        # Update the address counter\n",
    "        address_counter[address] += 1\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"Address\": address,\n",
    "            \"Building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"Size (sq. ft.)\": size,\n",
    "            \"Price\": price,\n",
    "            \"Bedrooms #\": beds,\n",
    "            \"Bathrooms # \": baths,\n",
    "            \"Url\": url\n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "# Call the parse_soup function to extract listing information\n",
    "all_listings = []  # List to store all dataframes\n",
    "\n",
    "for url in urls:\n",
    "    soup = soups(url)\n",
    "    listings = parse_soup(soup)\n",
    "    df = pd.DataFrame(listings)\n",
    "    all_listings.append(df)  # Append the dataframe to the list of dataframes\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(all_listings, ignore_index=True)\n",
    "\n",
    "# Filter out duplicates and ensure at least one unique occurrence of each address\n",
    "unique_addresses = [address for address, count in address_counter.items() if count == 1]\n",
    "unique_df = combined_df[combined_df['Address'].isin(unique_addresses)]\n",
    "\n",
    "# Export DataFrame to Excel file with date and page range in the file name\n",
    "date_created = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "excel_file_name = f\"unique_listings_{date_created}_pages_{start_page}_to_{end_page}.xlsx\"\n",
    "unique_df.to_excel(excel_file_name, index=False)\n",
    "print(f\"Unique DataFrame exported to {excel_file_name}\")\n",
    "\n",
    "# Display DataFrame with revised structure and formatting\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(unique_df)\n",
    "\n",
    "# Print the total number of unique addresses\n",
    "print(f\"Total number of unique addresses: {len(unique_addresses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some listings the information on number of bedroom, number of bathroom, and apartment size is incomplete or mixed up. I performed data manipulation to fix the mistaken values and clean up the extra symbols such as comma and dollar sign. <br\\ >\n",
    "Finally, I have two data sets containing the housing information for apartments for rent and apartments for sale. My for sale data set has 8,456 rows and 8 columns, and the for rent data set has 20,988 rows and 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#is the apartment furnished?\n",
    "cond=data['bed']=='Furnished'\n",
    "data.loc[cond,'furnished']=1\n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from size to bath\n",
    "cond=[]\n",
    "for i in data['size']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'size'] \n",
    "data.loc[cond,'size']=''\n",
    "\n",
    "#move from bed to bath\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='Furnished' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from bath to bed\n",
    "cond=[]\n",
    "for i in data['bath']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(len(i.split(\" \"))==1):\n",
    "            cond.append(True)\n",
    "        else:\n",
    "            if(i.split(\" \")[1] in ('bath','baths')):\n",
    "                cond.append(False)\n",
    "            else:\n",
    "                cond.append(True)\n",
    "data.loc[cond,'bed']=data.loc[cond,'bath'] \n",
    "data.loc[cond,'bath']=''\n",
    "\n",
    "#move from bed to size\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(i.split(\" \")[1] in ('bed','beds')):\n",
    "            cond.append(False)\n",
    "        else:\n",
    "            cond.append(True)\n",
    "data.loc[cond,'size']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "\n",
    "#replace blank with nan\n",
    "data=data.applymap(lambda x: np.nan if x=='' else x)\n",
    "\n",
    "#data\n",
    "data.to_csv('rent.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size to numeric\n",
    "cond=data['size'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'size']=int(data['size'][i].split(\" \")[0].replace(',',''))\n",
    "#bath to numeric\n",
    "cond=data['bath'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bath']=float(data['bath'][i].split(\" \")[0].replace('+',''))\n",
    "#bed to numeric\n",
    "cond=data['bed'].isnull()\n",
    "data['bed']=data['bed'].replace('studio','0 bed')\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bed']=float(data['bed'][i].split(\" \")[0].replace(',','').replace('+',''))\n",
    "#remove dollar sign\n",
    "data['price']=[int(i.replace('$','').replace(',','')) for i in data['price']]\n",
    "\n",
    "data.to_csv('rent_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
