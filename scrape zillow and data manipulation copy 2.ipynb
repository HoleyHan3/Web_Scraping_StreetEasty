{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Zillow.com to analyze housing price in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal here is to collect housing prices for both rental and sale in New York city. I looked at three major real estate website including Trulia, Zillow, and StreetEasy. Comparing to the other two websites, StreetEasy gives the most information on the searching results page and the format of each listing is very consistent, which is great for the purpose of web-scraping.<br\\ >\n",
    "<a href=\"http://zillow.com/\">\n",
    "<img \"StreetEasy\" src=\"map/streetEasy_logo.jpg\" height=\"30px\" width=\"150px\"/></a><br\\ >\n",
    "\n",
    "Web scraping is done using the beautifulsoup package in Python. I created two functions that can loop through all the pages of searching results, and also empty strings to store results. Below are the steps I took to scrape StreetEasy:\n",
    "1. Analyzing the HTML page: HTML code of a web page can be viewed by right click and selecting 'Inspect'. This helps us identifying the HTML tags of the information to be scraped\n",
    "2. Making the soup!: It is important to select the correct parser for your data type. I used HTML parser.\n",
    "3. Navigating the parse tree and iterate through tags: once the soup is made, we have the HTML code in Python. We can then find our desired information by searching through HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import lxml\n",
    "import numbers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://streeteasy.com/for-sale/nyc/']\n"
     ]
    }
   ],
   "source": [
    "req_headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-encoding': 'gzip, deflate, br',\n",
    "    'Accept-language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-insecure-requests': '1',\n",
    "    'User-agent': UserAgent().random,\n",
    "}\n",
    "\n",
    "#base_url = \"https://www.zillow.com/homes/for_sale/\"\n",
    "base_url = \"https://streeteasy.com/for-sale/\"\n",
    "urls = []\n",
    "\n",
    "city = 'nyc'\n",
    "url1 = base_url +city+'/'\n",
    "urls.append(url1)\n",
    "\n",
    "# Add all pages\n",
    "for i in range(2,2):\n",
    "    dom = base_url + city + '/' + 'page_' + str(i) #streeteasy\n",
    "    #dom = base_url + city + '/' + str(i) '_p' +'/' #zillow\n",
    "    if dom not in urls:\n",
    "        urls.append(dom)\n",
    "\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soups(data):\n",
    "    with requests.Session() as s:\n",
    "        r = s.get(data, headers=req_headers)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        print(soup.prettify())  # Corrected line: Print prettified HTML content from the soup object\n",
    "    return soup\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    htmls=soups(url)\n",
    "    print(htmls)\n",
    "# Call soup function and store output in a list\n",
    "#lst = []\n",
    "\n",
    "#for url in urls:\n",
    "#    htmls = soups(url)\n",
    "\n",
    "#print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store listing information\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Iterate through each listing card and extract relevant information\n",
    "            # Extract building type and neighborhood\n",
    "        building_info = card.find(class_=\"listingCardLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        # Extract address\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "\n",
    "        # Extract price\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        # Extract bed, bathroom details, and size\n",
    "        description = card.find('div', class_='description').text\n",
    "        bedrooms = card.find('div', class_='bedrooms').text\n",
    "        bathrooms = card.find('div', class_='bathrooms').text\n",
    "        size = card.find('div', class_='size').text\n",
    "\n",
    "        # Extract amenities match\n",
    "        amenities_list = card.find('ul', class_='amenities').find_all('li')\n",
    "        amenities = [amenity.text for amenity in amenities_list]\n",
    "\n",
    "        # property url\n",
    "        url = card.find('a', class_='listingCard-link jsCardLinkGA featured-link-to-hdp')['href']\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"address\": address,\n",
    "             \"building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"size (sq. ft.)\": size,\n",
    "            \"price\": price,\n",
    "            \"# bedrooms\": bedrooms,\n",
    "            \"# bathrooms\": bathrooms,\n",
    "            \"amenities\": amenities,\n",
    "            \"description\": description,\n",
    "            \"url\": url\n",
    "        \n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "        # Append the listing dictionary to the list of listings\n",
    "        \n",
    "            #address = data.find_all(class_= 'list-card-addr') zillow\n",
    "            #price = list(data.find_all(class_='list-card-price')) zillow\n",
    "            #beds = list(data.find_all(\"ul\", class_=\"list-card-details\")) zillow\n",
    "            #last_updated = data.find_all('div', {'class': 'list-card-top'}) zillow\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage:\n",
    "# Define the URL from which you want to fetch the HTML content\n",
    "#url = \"your_url_here\"\n",
    "\n",
    "# Fetch the HTML content from the URL\n",
    "#soup = soups(url)\n",
    "\n",
    "# Parse the HTML content and extract listing information from JSON-LD data\n",
    "listings = parse_soup(soup)\n",
    "\n",
    "# Create a pandas DataFrame from the extracted listing information\n",
    "df = pd.DataFrame(listings)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Address        Building_type_neighborhood  \\\n",
      "0        2286 Cropsey Avenue #18C                          Featured   \n",
      "1   2351 Adam Clayton Powell #201                          Featured   \n",
      "2           66 Madison Avenue #4E         Open House, Tue 5:30-6:30   \n",
      "3               89-15 96th Street               Verified up to date   \n",
      "4   500 West 18th Street EAST-11F                         Sponsored   \n",
      "5             36 Remsen Street #2               Verified up to date   \n",
      "6              2373 Broadway #511              Open House, Sun 12-1   \n",
      "7             56 Mc Arthur Avenue               Verified up to date   \n",
      "8       1020 Grand Concourse #11S  Contains \\n              3D Tour   \n",
      "9             138 Randolph Avenue               Verified up to date   \n",
      "10            1021 Boulevard East               Verified up to date   \n",
      "11          21-68 35th Street #5D               Verified up to date   \n",
      "12      235 East 22nd Street #10I               Open House, Sun 2-3   \n",
      "13          200 Hudson Street #4F               Verified up to date   \n",
      "\n",
      "                             Size (sq. ft.)       Price Bedrooms #  \\\n",
      "0     544\\n                square feet\\nft²    $628,000      1 Bed   \n",
      "1     801\\n                square feet\\nft²    $695,000      1 Bed   \n",
      "2                                      None    $665,000     2 Beds   \n",
      "3                                      None    $699,000     3 Beds   \n",
      "4   1,798\\n                square feet\\nft²  $4,850,000     2 Beds   \n",
      "5   1,800\\n                square feet\\nft²  $3,775,000     4 Beds   \n",
      "6                                      None  $1,350,000     2 Beds   \n",
      "7   3,078\\n                square feet\\nft²  $1,388,888     7 Beds   \n",
      "8   1,015\\n                square feet\\nft²    $399,000     2 Beds   \n",
      "9                                      None    $550,000     4 Beds   \n",
      "10                                     None  $2,200,000     7 Beds   \n",
      "11                                     None    $225,000      1 Bed   \n",
      "12  1,050\\n                square feet\\nft²  $1,295,000      1 Bed   \n",
      "13    788\\n                square feet\\nft²    $600,000      1 Bed   \n",
      "\n",
      "   Bathrooms #   \\\n",
      "0        1 Bath   \n",
      "1        1 Bath   \n",
      "2        1 Bath   \n",
      "3     1.5 Baths   \n",
      "4     2.5 Baths   \n",
      "5       2 Baths   \n",
      "6       2 Baths   \n",
      "7      2+ Baths   \n",
      "8        1 Bath   \n",
      "9       1+ Bath   \n",
      "10      5 Baths   \n",
      "11       1 Bath   \n",
      "12       1 Bath   \n",
      "13       1 Bath   \n",
      "\n",
      "                                                               Url  \n",
      "0                                          Property URL not found.  \n",
      "1                                          Property URL not found.  \n",
      "2                  https://streeteasy.com/building/madison-parq/4e  \n",
      "3                              https://streeteasy.com/sale/1700271  \n",
      "4   https://streeteasy.com/building/one-high-line/east11f?infeed=1  \n",
      "5      https://streeteasy.com/building/36-remsen-street-brooklyn/2  \n",
      "6       https://streeteasy.com/building/the-boulevard/sale/1637890  \n",
      "7                              https://streeteasy.com/sale/1700266  \n",
      "8   https://streeteasy.com/building/1020-grand-concourse-bronx/11s  \n",
      "9                              https://streeteasy.com/sale/1700264  \n",
      "10                             https://streeteasy.com/sale/1700263  \n",
      "11      https://streeteasy.com/building/21_68-35-street-astoria/5d  \n",
      "12              https://streeteasy.com/building/gramercy-house/10i  \n",
      "13    https://streeteasy.com/building/200-hudson-street-hoboken/4f  \n",
      "DataFrame exported to listings.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "def soups(data):\n",
    "    with requests.Session() as s:\n",
    "        r = s.get(data, headers=req_headers)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        #print(soup.prettify())  # Corrected line: Print prettified HTML content from the soup object\n",
    "    return soup\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Iterate through each listing card and extract relevant information\n",
    "            # Extract building type and neighborhood\n",
    "        building_info = card.find(class_=\"listingCardLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        # Extract address\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "\n",
    "        # Extract price\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        # Extract bed information\n",
    "        bed_elem = card.find('span', class_='listingDetailDefinitionsIcon--bed')\n",
    "        beds = bed_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bed_elem else None\n",
    "\n",
    "        # Extract bathroom information\n",
    "        bath_elem = card.find('span', class_='listingDetailDefinitionsIcon--bath')\n",
    "        baths = bath_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bath_elem else None\n",
    "   \n",
    "        size_elem = card.find('span', class_='listingDetailDefinitionsIcon--measure')\n",
    "        size = size_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if size_elem else None\n",
    "\n",
    "        # Extract amenities match\n",
    "    #    amenities_list = card.find('ul', class_='amenities').find_all('li')\n",
    "    #    amenities = [amenity.text for amenity in amenities_list]\n",
    "\n",
    "        # Extract property URL\n",
    "        url_element = card.find('a', class_='listingCard-link jsCardLinkGA')\n",
    "        url = url_element.get('href') if url_element else 'Property URL not found.'\n",
    "\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"Address\": address,\n",
    "            \"Building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"Size (sq. ft.)\": size,\n",
    "            \"Price\": price,\n",
    "            \"Bedrooms #\": beds,\n",
    "            \"Bathrooms # \": baths,\n",
    "        #    \"amenities\": amenities,\n",
    "            \"Url\": url\n",
    "        \n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "# Call the parse_soup function to extract listing information\n",
    "for url in urls:\n",
    "    soup = soups(url)\n",
    "    listings = parse_soup(soup)\n",
    "    df = pd.DataFrame(listings)\n",
    "\n",
    "    # Display DataFrame with revised structure and formatting\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(df)\n",
    "    # Export DataFrame to Excel file\n",
    "    \n",
    "    excel_file_name = \"listings.xlsx\"\n",
    "    df.to_excel(excel_file_name, index=False)\n",
    "    print(f\"DataFrame exported to {excel_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_df = pd.DataFrame()\n",
    "\n",
    "# Get our data from each url and put it into a list of dataframes\n",
    "df_list = []\n",
    "for soup in lst:\n",
    "    new_df = my_soup(soup)\n",
    "    df_list.append(new_df)\n",
    "    \n",
    "# Combine the list of datasets into our new dataframe\n",
    "zillow_df = pd.concat(df_list)\n",
    "zillow_df.reset_index(inplace=True)\n",
    "zillow_df = zillow_df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zillow_df.shape)\n",
    "zillow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some listings the information on number of bedroom, number of bathroom, and apartment size is incomplete or mixed up. I performed data manipulation to fix the mistaken values and clean up the extra symbols such as comma and dollar sign. <br\\ >\n",
    "Finally, I have two data sets containing the housing information for apartments for rent and apartments for sale. My for sale data set has 8,456 rows and 8 columns, and the for rent data set has 20,988 rows and 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#is the apartment furnished?\n",
    "cond=data['bed']=='Furnished'\n",
    "data.loc[cond,'furnished']=1\n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from size to bath\n",
    "cond=[]\n",
    "for i in data['size']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'size'] \n",
    "data.loc[cond,'size']=''\n",
    "\n",
    "#move from bed to bath\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='Furnished' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from bath to bed\n",
    "cond=[]\n",
    "for i in data['bath']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(len(i.split(\" \"))==1):\n",
    "            cond.append(True)\n",
    "        else:\n",
    "            if(i.split(\" \")[1] in ('bath','baths')):\n",
    "                cond.append(False)\n",
    "            else:\n",
    "                cond.append(True)\n",
    "data.loc[cond,'bed']=data.loc[cond,'bath'] \n",
    "data.loc[cond,'bath']=''\n",
    "\n",
    "#move from bed to size\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(i.split(\" \")[1] in ('bed','beds')):\n",
    "            cond.append(False)\n",
    "        else:\n",
    "            cond.append(True)\n",
    "data.loc[cond,'size']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "\n",
    "#replace blank with nan\n",
    "data=data.applymap(lambda x: np.nan if x=='' else x)\n",
    "\n",
    "#data\n",
    "data.to_csv('rent.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size to numeric\n",
    "cond=data['size'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'size']=int(data['size'][i].split(\" \")[0].replace(',',''))\n",
    "#bath to numeric\n",
    "cond=data['bath'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bath']=float(data['bath'][i].split(\" \")[0].replace('+',''))\n",
    "#bed to numeric\n",
    "cond=data['bed'].isnull()\n",
    "data['bed']=data['bed'].replace('studio','0 bed')\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bed']=float(data['bed'][i].split(\" \")[0].replace(',','').replace('+',''))\n",
    "#remove dollar sign\n",
    "data['price']=[int(i.replace('$','').replace(',','')) for i in data['price']]\n",
    "\n",
    "data.to_csv('rent_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
