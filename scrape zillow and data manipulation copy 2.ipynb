{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Zillow.com to analyze housing price in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal here is to collect housing prices for both rental and sale in New York city. I looked at three major real estate website including Trulia, Zillow, and StreetEasy. Comparing to the other two websites, StreetEasy gives the most information on the searching results page and the format of each listing is very consistent, which is great for the purpose of web-scraping.<br\\ >\n",
    "<a href=\"http://zillow.com/\">\n",
    "<img \"StreetEasy\" src=\"map/streetEasy_logo.jpg\" height=\"30px\" width=\"150px\"/></a><br\\ >\n",
    "\n",
    "Web scraping is done using the beautifulsoup package in Python. I created two functions that can loop through all the pages of searching results, and also empty strings to store results. Below are the steps I took to scrape StreetEasy:\n",
    "1. Analyzing the HTML page: HTML code of a web page can be viewed by right click and selecting 'Inspect'. This helps us identifying the HTML tags of the information to be scraped\n",
    "2. Making the soup!: It is important to select the correct parser for your data type. I used HTML parser.\n",
    "3. Navigating the parse tree and iterate through tags: once the soup is made, we have the HTML code in Python. We can then find our desired information by searching through HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import lxml\n",
    "import numbers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req_headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-encoding': 'gzip, deflate, br',\n",
    "    'Accept-language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-insecure-requests': '1',\n",
    "    'User-agent': UserAgent().random,\n",
    "}\n",
    "\n",
    "base_url = \"https://streeteasy.com/for-sale/\"\n",
    "#base_url = \"https://www.zillow.com/homes/for_sale/\"\n",
    "urls = []\n",
    "\n",
    "city = 'nyc'\n",
    "url1 = base_url +city+'/'\n",
    "urls.append(url1)\n",
    "\n",
    "start_page = 2\n",
    "end_page = 3\n",
    "\n",
    "# Add all pages\n",
    "for i in range(start_page, end_page + 1):\n",
    "    dom = base_url + city + '/' + 'page_' + str(i) #streeteasy\n",
    "    #dom = base_url + city + '/' + str(i) '_p' +'/' #zillow\n",
    "    if dom not in urls:\n",
    "        urls.append(dom)\n",
    "\n",
    "print(urls)\n",
    "\n",
    "#https://streeteasy.com/for-sale/nyc/price:-400000?page=2 with max\n",
    "#https://streeteasy.com/for-sale/nyc/price:100000-700000 min and max\n",
    "#https://streeteasy.com/for-rent/nyc/price:-12500 for rent\n",
    "#https://streeteasy.com/for-rent/nyc/page_2 for rent\n",
    "\n",
    "# Define the rate limit: e.g., 5 calls per 60 seconds\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=60)\n",
    "def soups(data):\n",
    "    with requests.Session() as s:\n",
    "        r = s.get(data, headers=req_headers)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        #print(soup.prettify())  # Corrected line: Print prettified HTML content from the soup object\n",
    "    return soup\n",
    "\n",
    "\n",
    "#for url in urls:\n",
    "#    htmls=soups(url)\n",
    "# Call soup function and store output in a list\n",
    "#lst = []\n",
    "\n",
    "#for url in urls:\n",
    "#    htmls = soups(url)\n",
    "\n",
    "#print(len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches and saves prettify data from rental site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=60)\n",
    "def get_page_content(url,headers):\n",
    "    \"\"\"\n",
    "    Function to get the HTML content of a webpage given its URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page content: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_content_to_file(content, filename):\n",
    "    \"\"\"\n",
    "    Function to write content to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def main():\n",
    "    # Define the base URL and the number of pages you want to scrape\n",
    "    #base_url = \"https://zillow.com/new-york-ny/rentals\"\n",
    "    #base_url = \"https://zillow.com/new-york-ny/sales\"\n",
    "    #base_url = \"https://zillow.com/new-york-ny/sold\"\n",
    "    #base_url = \"http://streeteasy.com/for-sale/nyc?\" #1272 \n",
    "    base_url = \"http://streeteasy.com/for-rent/nyc?\" #1392\n",
    "    num_pages = 1392  # Number of additional pages to scrape\n",
    "    start_page = 1  # Starting page number\n",
    "    headers = {\n",
    "        'User-Agent': UserAgent().random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://google.com',\n",
    "        'Pragma': 'no-cache',\n",
    "        'Cache-Control': 'no-cache'\n",
    "    }\n",
    "\n",
    "    # Request and parse content for each page\n",
    "    for page_number in range(start_page, num_pages + 1):\n",
    "        #full_url = f\"{base_url}/{page_number}_p/\" Zillow        \n",
    "        full_url = f\"{base_url}page={page_number}\" # Streeteasy\n",
    "        print(f\"Fetching content from: {full_url}\")\n",
    "        page_content = get_page_content(full_url, headers)\n",
    "        if page_content:\n",
    "            # Write content to a file\n",
    "            #filename = f\"Page_{page_number}.html\"  # Create a filename based on the page number\n",
    "            #print(f\"Page {page_number} content fetched successfully.\")\n",
    "           # write_content_to_file(page_content, filename)\n",
    "           # print(f\"Prettified content written to: {filename}\")\n",
    "\n",
    "             # Parse content using BeautifulSoup and prettify\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            prettified_content = soup.prettify()\n",
    "            \n",
    "            # Write content to a file\n",
    "            prettyFilename = f\"pretty_page_{page_number}.html\"  # Create a filename based on the page number\n",
    "            write_content_to_file(prettified_content, prettyFilename)\n",
    "            print(f\"Prettified content written to: {prettyFilename}\")\n",
    "        else:\n",
    "            print(f\"No content found for: {full_url}\")\n",
    "        print(\"=\" * 50)  # Separator for better readability\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "rate_limit = 5 #calls/pages per minute\n",
    "period = 60 #a period of 60 seconds\n",
    "\n",
    "def estimate_time(rate_limit, period):\n",
    "    time_needed = pages / rate_limit #pages/5calls/minute\n",
    "    minutes = math.floor(time_needed)\n",
    "    seconds = math.ceil((time_needed - minutes) * 60)\n",
    "    return minutes, seconds\n",
    "\n",
    "# Example usage:\n",
    "pages = 98\n",
    "\n",
    "minutes, seconds = estimate_time(pages, rate_limit)\n",
    "print(f\"Estimated time needed: {minutes} minutes and {seconds} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "\n",
    "# Web URL\n",
    "web_url = \"https://www.zillow.com/new-york-ny/?searchQueryState=%7B%22pagination%22%3A%7B%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22west%22%3A-76.09592245507815%2C%22east%22%3A-71.8634395449219%2C%22south%22%3A40.120647691241565%2C%22north%22%3A41.27009728054556%7D%2C%22usersSearchTerm%22%3A%22New%20York%2C%20NY%22%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A6181%2C%22regionType%22%3A6%7D%5D%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22globalrelevanceex%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A9%7D\"\n",
    "# https://www.zillow.com/new-york-ny/for_sale/3_p/\n",
    "# Define request headers\n",
    "req_headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-encoding': 'gzip, deflate, br',\n",
    "    'Accept-language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-insecure-requests': '1',\n",
    "    'User-agent': UserAgent().random,\n",
    "}\n",
    "\n",
    "\n",
    "# Get URL Content\n",
    "r = requests.get(web_url, headers=req_headers)\n",
    "\n",
    "# Parse HTML Code\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "all_data = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
    "\n",
    "data = []\n",
    "for ld_json in all_data:\n",
    "    jsn = json.loads(ld_json.string)\n",
    "    data.append(jsn)\n",
    "    print(jsn)\n",
    "\n",
    "response_content = fetch_zillow_data(url)\n",
    "if response_content:\n",
    "    properties = extract_json_data_property_info(response_content)\n",
    "    for property in properties:\n",
    "        print(property)\n",
    "\n",
    "def extract_json_data_property_info(response_content):\n",
    "    \"\"\"\n",
    "    Extracts property information from Zillow data.\n",
    "    \n",
    "    Parameters:\n",
    "    - response_content (str): The content of the HTTP response containing Zillow data.\n",
    "    \n",
    "    Returns:\n",
    "    - property_info (list): A list containing dictionaries with property information.\n",
    "    \"\"\"\n",
    "    if response_content is None:\n",
    "        return []\n",
    "\n",
    "    property_info = []\n",
    "    soup = BeautifulSoup(response_content, \"html.parser\")\n",
    "    scripts = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
    "    \n",
    "    for script in scripts:\n",
    "        data = json.loads(script.string)\n",
    "        property_type = data.get(\"@type\", \"\")\n",
    "        street_address = data[\"address\"][\"streetAddress\"] if \"address\" in data else \"\"\n",
    "        locality = data[\"address\"][\"addressLocality\"] if \"address\" in data else \"\"\n",
    "        region = data[\"address\"][\"addressRegion\"] if \"address\" in data else \"\"\n",
    "        postal_code = data[\"address\"][\"postalCode\"] if \"address\" in data else \"\"\n",
    "        floor_size = data[\"floorSize\"][\"value\"] if \"floorSize\" in data else \"\"\n",
    "        latitude = data[\"geo\"][\"latitude\"] if \"geo\" in data else \"\"\n",
    "        longitude = data[\"geo\"][\"longitude\"] if \"geo\" in data else \"\"\n",
    "        url = data.get(\"url\", \"\")\n",
    "        \n",
    "        property_info.append({\n",
    "            \"property_type\": property_type,\n",
    "            \"floor_size\": floor_size,\n",
    "            \"street_address\": street_address,\n",
    "            \"locality\": locality,\n",
    "            \"region\": region,\n",
    "            \"postal_code\": postal_code,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"url\": url\n",
    "        })\n",
    "\n",
    "    return property_info\n",
    "\n",
    "\n",
    "#print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store listing information\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Iterate through each listing card and extract relevant information\n",
    "            # Extract building type and neighborhood\n",
    "        building_info = card.find(class_=\"listingCardLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        # Extract address\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "\n",
    "        # Extract price\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        # Extract bed, bathroom details, and size\n",
    "        description = card.find('div', class_='description').text\n",
    "        bedrooms = card.find('div', class_='bedrooms').text\n",
    "        bathrooms = card.find('div', class_='bathrooms').text\n",
    "        size = card.find('div', class_='size').text\n",
    "\n",
    "        # Extract amenities match\n",
    "        amenities_list = card.find('ul', class_='amenities').find_all('li')\n",
    "        amenities = [amenity.text for amenity in amenities_list]\n",
    "\n",
    "        # property url\n",
    "        url = card.find('a', class_='listingCard-link jsCardLinkGA featured-link-to-hdp')['href']\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"address\": address,\n",
    "             \"building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"size (sq. ft.)\": size,\n",
    "            \"price\": price,\n",
    "            \"# bedrooms\": bedrooms,\n",
    "            \"# bathrooms\": bathrooms,\n",
    "            \"amenities\": amenities,\n",
    "            \"description\": description,\n",
    "            \"url\": url\n",
    "        \n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "        # Append the listing dictionary to the list of listings\n",
    "        \n",
    "            #address = data.find_all(class_= 'list-card-addr') zillow\n",
    "            #price = list(data.find_all(class_='list-card-price')) zillow\n",
    "            #beds = list(data.find_all(\"ul\", class_=\"list-card-details\")) zillow\n",
    "            #last_updated = data.find_all('div', {'class': 'list-card-top'}) zillow\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage:\n",
    "# Define the URL from which you want to fetch the HTML content\n",
    "#url = \"your_url_here\"\n",
    "\n",
    "# Fetch the HTML content from the URL\n",
    "#soup = soups(url)\n",
    "\n",
    "# Parse the HTML content and extract listing information from JSON-LD data\n",
    "listings = parse_soup(soup)\n",
    "\n",
    "# Create a pandas DataFrame from the extracted listing information\n",
    "df = pd.DataFrame(listings)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_zillow_data(url, cache_file):\n",
    "    \"\"\"\n",
    "    Fetches Zillow data from the provided URL, either by fetching it from the cache or making a new request.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL to fetch Zillow data from.\n",
    "    - cache_file (str): The file path for caching the fetched data.\n",
    "\n",
    "    Returns:\n",
    "    - response_content (str): The content of the HTTP response containing Zillow data.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        headers = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-encoding': 'gzip, deflate, br',\n",
    "            'Accept-language': 'en-US,en;q=0.8',\n",
    "            'Upgrade-insecure-requests': '1',\n",
    "            'User-agent': UserAgent().random,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            response_content = response.content\n",
    "            with open(cache_file, 'w') as f:\n",
    "                f.write(response_content.decode('utf-8'))\n",
    "            return response_content\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def extract_property_info(response_content):\n",
    "    property_info = []\n",
    "    \"\"\"\n",
    "    Extracts property information from the response content.\n",
    "\n",
    "    Parameters:\n",
    "    - response_content (str): The content of the HTTP response containing Zillow data.\n",
    "\n",
    "    Returns:\n",
    "    - property_info (list): List of dictionaries containing unique property information.\n",
    "    \n",
    "    if not response_content:\n",
    "        print(\"Error: Response content is empty.\")\n",
    "        return []\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(response_content, \"html.parser\")\n",
    "        zpid_elements = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
    "\n",
    "        #Extracting data from the __NEXT_DATA__ script tag\n",
    "        #next_data_script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "        #if next_data_script:\n",
    "            #try:\n",
    "                #next_data = json.loads(next_data_script.string)\n",
    "                # Prettify the JSON data\n",
    "                #prettified_data = json.dumps(next_data, indent=2)\n",
    "                #print(prettified_data)\n",
    "                # Access and process the data as needed\n",
    "                # Example: data = next_data['props']['pageProps']['your_data_key']\n",
    "            #except json.JSONDecodeError as e:\n",
    "                #print(f\"Error decoding JSON from __NEXT_DATA__: {e}\")\n",
    "\n",
    "        for script in zpid_elements:\n",
    "            data = json.loads(script.string)\n",
    "            url = data.get(\"url\", \"\")\n",
    "            zpid_parts = url.split(\"/\")\n",
    "            if len(zpid_parts) >= 2:\n",
    "                zpid_str = zpid_parts[-2]\n",
    "                zpid_numeric = ''.join(filter(str.isdigit, zpid_str))\n",
    "                zpid = int(zpid_numeric) if zpid_numeric else 0\n",
    "            else:\n",
    "                zpid = 0\n",
    "\n",
    "                property_details = {\n",
    "                \"zpid\": zpid,\n",
    "                \"property_type\": \"\",\n",
    "                \"price\": \"\",\n",
    "                \"street_address\": \"\",\n",
    "                \"locality\": \"\",\n",
    "                \"region\": \"\",\n",
    "                \"postal_code\": \"\",\n",
    "                \"floor_size\": \"\",\n",
    "                \"bedrooms\": \"\",\n",
    "                \"bathrooms\": \"\",\n",
    "                \"sqft\": \"\",\n",
    "                \"days_on_zillow\": \"\",\n",
    "                \"latitude\": \"\",\n",
    "                \"longitude\": \"\",\n",
    "                \"url\": url,\n",
    "            }\n",
    "                \n",
    "            # Extract property details from JSON data\n",
    "            if \"@type\" in data:\n",
    "                address_data = data.get(\"address\", {})\n",
    "                property_details[\"street_address\"] = address_data.get(\"streetAddress\", \"\")\n",
    "                property_details[\"locality\"] = address_data.get(\"addressLocality\", \"\")\n",
    "                property_details[\"region\"] = address_data.get(\"addressRegion\", \"\")\n",
    "                property_details[\"postal_code\"] = address_data.get(\"postalCode\", \"\")\n",
    "\n",
    "            if \"floorSize\" in data and not property_details[\"floor_size\"]:\n",
    "                property_details[\"floor_size\"] = ''.join(filter(str.isdigit, data.get(\"floorSize\", {}).get(\"value\", \"\")))\n",
    "\n",
    "            if \"geo\" in data:\n",
    "                property_details[\"latitude\"] = data[\"geo\"].get(\"latitude\", \"\")\n",
    "                property_details[\"longitude\"] = data[\"geo\"].get(\"longitude\", \"\")\n",
    "\n",
    "            # Additional property details extraction if needed\n",
    "            corresponding_html = soup.find(\"article\", {\"id\": lambda x: x and x.startswith(f\"zpid_{zpid}\")})\n",
    "            if corresponding_html:\n",
    "                if not property_details[\"street_address\"]:\n",
    "                    address_element = corresponding_html.find(\"address\", {\"data-test\": \"property-card-addr\"})\n",
    "                    if address_element:\n",
    "                        address = address_element.get_text(strip=True)\n",
    "                        property_details[\"street_address\"] = address\n",
    "\n",
    "                if not property_details[\"price\"]:\n",
    "                    price_element = corresponding_html.find(\"span\", {\"data-test\": \"property-card-price\"})\n",
    "                    if price_element:\n",
    "                        property_details[\"price\"] = price_element.get_text(strip=True)\n",
    "\n",
    "                ul_elements = corresponding_html.find_all(\"ul\", class_=\"StyledPropertyCardHomeDetailsList-c11n-8-84-3__sc-1xvdaej-0\")\n",
    "                for ul in ul_elements:\n",
    "                    lis = ul.find_all(\"li\")\n",
    "                    for li in lis:\n",
    "                        abbr = li.find(\"abbr\")\n",
    "                        if abbr:\n",
    "                            abbr_text = abbr.text.strip().lower()\n",
    "                            if \"bd\" in abbr_text:\n",
    "                                property_details[\"bedrooms\"] = li.find(\"b\").text.strip()\n",
    "                            elif \"ba\" in abbr_text:\n",
    "                                property_details[\"bathrooms\"] = li.find(\"b\").text.strip()\n",
    "                            elif \"sqft\" in abbr_text:\n",
    "                                property_details[\"sqft\"] = li.find(\"b\").text.strip()                                \n",
    "\n",
    "                # Extracting property types\n",
    "                div_elements = corresponding_html.find_all(\"div\", class_=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\")\n",
    "                for div in div_elements:\n",
    "                    text_content = div.get_text().strip()            \n",
    "\n",
    "                    # Split the text on \"-\" character and take the first part as the property type\n",
    "                    parts = text_content.split(\"-\")\n",
    "                    if len(parts) > 1:\n",
    "                        property_type = parts[0].strip()\n",
    "                    else:\n",
    "                        # If the text doesn't contain \"-\", use the entire text as the property type\n",
    "                        property_type = text_content.strip()\n",
    "\n",
    "                    # Append the extracted property type to the property_types list\n",
    "                    property_details[\"property_type\"] = property_type\n",
    "\n",
    "\n",
    "                # Extracting days on Zillow\n",
    "                days_on_zillow_element = corresponding_html.find(\"span\", class_=\"StyledPropertyCardBadge-c11n-8-84-3__sc-6gojrl-0\")\n",
    "                if days_on_zillow_element:\n",
    "                    property_details[\"days_on_zillow\"] = days_on_zillow_element.text.strip()\n",
    "\n",
    "\n",
    "                # Validate property details\n",
    "                if property_details[\"zpid\"] != 0 and property_details[\"street_address\"] and property_details[\"price\"]:\n",
    "                    property_info.append(property_details)\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON data: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during extraction: {e}\")\n",
    "\n",
    "    return property_info\n",
    "\n",
    "\n",
    "#    url = f\"{base_url}{page_number}_p/?searchQueryState=%7B%22pagination%22%3A%7B%22currentPage%22%3A{page_number}%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22west%22%3A-74.92725180078128%2C%22east%22%3A-73.03211019921878%2C%22south%22%3A40.27379533285657%2C%22north%22%3A41.11922552941539%7D%2C%22mapZoom%22%3A9%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A6181%2C%22regionType%22%3A6%7D%5D%2C%22filterState%22%3A%7B%22fr%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%7D\"\n",
    "\n",
    "\n",
    "#dfs = []  # List to store DataFrames\n",
    "\n",
    "def main():\n",
    "    rental_base_url = \"https://www.zillow.com/new-york-ny/rentals/\"\n",
    "    sales_base_url = \"https://www.zillow.com/new-york-ny/sales/\"\n",
    "    urls = []\n",
    "\n",
    "    pages_to_fetch = 2  # Adjust the number of pages to fetch\n",
    "\n",
    "    for i in range(1, pages_to_fetch + 1):\n",
    "        if i == 1:\n",
    "           urls.append(rental_base_url) #for rentals\n",
    "            #urls.append(sales_base_url) #for sales\n",
    "        else:\n",
    "            rental_url = f\"{rental_base_url}{i}_p/\"#?searchQueryState=%7B%22pagination%22%3A%7B%22currentPage%22%3A{i}%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22west%22%3A-74.92725180078128%2C%22east%22%3A-73.03211019921878%2C%22south%22%3A40.3962734787787%2C%22north%22%3A40.99807055129214%7D%2C%22mapZoom%22%3A9%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A6181%2C%22regionType%22%3A6%7D%5D%2C%22filterState%22%3A%7B%22fr%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%7D\"\n",
    "            #sales_url = f\"{sales_base_url}{i}_p/\"\n",
    "            urls.append(rental_url)\n",
    "            #urls.append(sales_url)\n",
    "\n",
    "    print(\"Number of URLs to fetch:\", len(urls))\n",
    "    print(\"URLs to fetch:\")\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "\n",
    "    # Iterate through the URLs and fetch Zillow data\n",
    "    for url in urls:\n",
    "        cache_file = f\"zillow_cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
    "        response_content = fetch_zillow_data(url, cache_file)\n",
    "\n",
    "        if response_content:\n",
    "            # Extract property information\n",
    "            property_info = extract_property_info(response_content)\n",
    "\n",
    "            # Display the extracted property information\n",
    "            print(f\"Property Information for {url}:\")\n",
    "            for property_details in property_info:\n",
    "                print(property_details)\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "             # Prettify and save the response content as a .txt file\n",
    "            prettified_content = BeautifulSoup(response_content, \"html.parser\").prettify()\n",
    "            txt_file_name = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_zillow_page.txt\"\n",
    "            with open(txt_file_name, \"w\") as txt_file:\n",
    "                txt_file.write(prettified_content)\n",
    "            print(f\"Prettified HTML content saved as {txt_file_name}\")\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "''' \n",
    "for web_url in urls:\n",
    "    cache_file = \"zillow_cache.html\"\n",
    "    if response_content:\n",
    "        response_content = fetch_zillow_data(web_url, cache_file)\n",
    "        properties = extract_property_info(response_content)\n",
    "        soup = BeautifulSoup(response_content, 'html.parser')\n",
    "        #prettified_data = soup.prettify(response_content)\n",
    "        #print(soup.prettify())\n",
    "\n",
    "\n",
    "\n",
    "#    \n",
    "#        \n",
    "#        df = pd.DataFrame(properties)\n",
    "#        dfs.append(df)  # Append each DataFrame to the list\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "#    if dfs:\n",
    "#        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "#        print(combined_df)\n",
    "#        csv_file_title = f\"{len(combined_df)}_listings_{datetime.now().strftime('%Y-%m-%d_%H-%M')}.csv\"\n",
    "#        combined_df.to_csv(csv_file_title, index=False)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Define a defaultdict to store the occurrence count of each address\n",
    "address_counter = defaultdict(int)\n",
    "\n",
    "def extract_numeric_value(text):\n",
    "    if text is not None:\n",
    "        # Use regular expression to extract numerical values, including optional units\n",
    "        numeric_value = re.search(r'(\\d{1,3}(,\\d{3})*(\\.\\d+)?)\\s*(?:square\\s*feet)?', text)\n",
    "        if numeric_value:\n",
    "            # Remove commas from the extracted value\n",
    "            numeric_value_without_commas = numeric_value.group(1).replace(',', '')\n",
    "            return numeric_value_without_commas  # Return the extracted numeric value\n",
    "    return None\n",
    "\n",
    "# Function to parse the HTML soup\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Extract relevant information from each listing card\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "        \n",
    "        building_info = card.find('p',class_=\"listingCardLabel listingCardLabel-grey listingCard-upperShortLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        bed_elem = card.find('span', class_='listingDetailDefinitionsIcon--bed')\n",
    "        beds_text = bed_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bed_elem else None\n",
    "        beds = extract_numeric_value(beds_text)\n",
    "\n",
    "        bath_elem = card.find('span', class_='listingDetailDefinitionsIcon--bath')\n",
    "        baths_text = bath_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bath_elem else None\n",
    "        baths = extract_numeric_value(baths_text)\n",
    "\n",
    "        # Extract size\n",
    "        size_elem = card.find('span', class_='listingDetailDefinitionsIcon--measure')\n",
    "        size_text = size_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if size_elem else None\n",
    "        size = extract_numeric_value(size_text)\n",
    "\n",
    "        url_element = card.find('a', class_='listingCard-link jsCardLinkGA')\n",
    "        url = url_element.get('href') if url_element else 'Property URL not found.'\n",
    "\n",
    "        # Update the address counter\n",
    "        address_counter[address] += 1\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"Address\": address,\n",
    "            \"Building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"Size (sq. ft.)\": size,\n",
    "            \"Price\": price,\n",
    "            \"Bedrooms #\": beds,\n",
    "            \"Bathrooms # \": baths,\n",
    "            \"Url\": url\n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "# Call the parse_soup function to extract listing information\n",
    "all_listings = []  # List to store all dataframes\n",
    "\n",
    "for url in urls:\n",
    "    soup = soups(url)\n",
    "    listings = parse_soup(soup)\n",
    "    df = pd.DataFrame(listings)\n",
    "    all_listings.append(df)  # Append the dataframe to the list of dataframes\n",
    "\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(all_listings, ignore_index=True)\n",
    "# Print the concatenated DataFrame\n",
    "\n",
    "# Display DataFrame with revised structure and formatting\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# Filter out duplicates and ensure at least one unique occurrence of each address\n",
    "unique_addresses = [address for address, count in address_counter.items() if count == 1]\n",
    "print(f\"Total number of unique addresses: {len(unique_addresses)}\")\n",
    "unique_df = combined_df[combined_df['Address'].isin(unique_addresses)]\n",
    "\n",
    "# Export DataFrame to Excel file with date and page range in the file name\n",
    "date_created = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "excel_file_name = f\"unique_listings_{date_created}_pages_{start_page}_to_{end_page}.xlsx\"\n",
    "#combined_df.to_excel(excel_file_name, index=False)\n",
    "unique_df.to_excel(excel_file_name, index=False)\n",
    "print(f\"Unique DataFrame exported to {excel_file_name}\")\n",
    "print(unique_df)\n",
    "\n",
    "#print(\"Unique DataFrame:\")\n",
    "#print(unique_df)\n",
    "\n",
    "# Print the total number of unique addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os   \n",
    "\n",
    "# Directory containing the HTML files\n",
    "directory = 'Pretty_Resources/Zillow/Zillow Rentals/'\n",
    "\n",
    "# Initialize empty lists or dictionaries to store aggregated data\n",
    "all_address_details = {}\n",
    "all_listing_info = []\n",
    "\n",
    "# Function to parse HTML content from a file\n",
    "def parse_html_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "    return html_content\n",
    "\n",
    "def extract_home_details(html_content):\n",
    "    \"\"\"\n",
    "    Extracts unique home details corresponding to each address from the provided HTML content.\n",
    "\n",
    "    Args:\n",
    "    - html_content (str): The HTML content to parse.\n",
    "\n",
    "    Returns:\n",
    "    - address_details_dict (dict): A dictionary where keys are addresses and values are lists of unique home details.\n",
    "    \"\"\"\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all list items within the main ul\n",
    "    list_items = soup.find_all('li', class_='ListItem-c11n-8-84-3__sc-10e22w8-0 StyledListCardWrapper-srp__sc-wtsrtn-0 iCyebE gTOWtl')\n",
    "    \n",
    "    # Initialize a dictionary to store address-details pairs\n",
    "    address_details_dict = {}\n",
    "    \n",
    "    # Iterate through the list items\n",
    "    for item in list_items:\n",
    "        # Extract address information\n",
    "        address_tag = item.find('a', class_='StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 jnnxAW property-card-link')\n",
    "        if address_tag:\n",
    "            address_info = address_tag.get_text(strip=True)\n",
    "        \n",
    "        # Find the ul containing home details within each list item\n",
    "        home_details_ul = item.find('ul', class_='StyledPropertyCardHomeDetailsList-c11n-8-84-3__sc-1xvdaej-0 eYPFID')\n",
    "        if home_details_ul:\n",
    "            # Extract and print the text content of each li within the ul\n",
    "            home_details = [li.get_text(strip=True) for li in home_details_ul.find_all('li')]\n",
    "            # Add details to the dictionary corresponding to the address\n",
    "            if address_info in address_details_dict:\n",
    "                address_details_dict[address_info].extend(home_details)\n",
    "            else:\n",
    "                address_details_dict[address_info] = home_details\n",
    "\n",
    "    # Remove duplicates from the details lists\n",
    "    for address, details in address_details_dict.items():\n",
    "        address_details_dict[address] = list(set(details))\n",
    "\n",
    "    return address_details_dict\n",
    "\n",
    "\n",
    "def extract_listing_info(html_content):\n",
    "    \"\"\"\n",
    "    Extracts information from various types of script tags containing JSON data.\n",
    "\n",
    "    Args:\n",
    "    - html_content (str): The HTML content to parse.\n",
    "\n",
    "    Returns:\n",
    "    - json_list (list): A list of dictionaries containing information from each JSON content.\n",
    "    \"\"\"\n",
    "    json_list = []\n",
    "\n",
    "    try:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all <script> tags\n",
    "        script_tags = soup.find_all('script')\n",
    "\n",
    "        for script_tag in script_tags:\n",
    "            script_content = script_tag.string\n",
    "\n",
    "            if script_content:\n",
    "                # Check if the script contains JSON data\n",
    "                try:\n",
    "                    json_content = json.loads(script_content)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip if not JSON data\n",
    "\n",
    "                # Extract data based on the script type\n",
    "                if 'name' in json_content and 'startDate' in json_content:\n",
    "                    # This is likely event data\n",
    "                    event_info = {\n",
    "                        'name': json_content.get('name', ''),\n",
    "                    #    'startDate': json_content.get('startDate', ''),\n",
    "                    #    'endDate': json_content.get('endDate', ''),\n",
    "                        'url': json_content.get('url', ''),\n",
    "                    #    'image': json_content.get('image', ''),\n",
    "                    #    'location': json_content.get('location', [])\n",
    "                    }\n",
    "                    json_list.append(event_info)\n",
    "                elif 'props' in json_content and 'pageProps' in json_content['props']:\n",
    "                    # This may contain page structure and search results\n",
    "                    page_props = json_content['props']['pageProps']\n",
    "                    if 'searchResults' in page_props:\n",
    "                        search_results = page_props['searchResults'].get('listResults', [])\n",
    "                        for result in search_results:\n",
    "                            result_info = {\n",
    "                                'zpid': result.get('zpid', ''),\n",
    "                                'property_type': result.get('statusType', ''),\n",
    "                                'address': result.get('address', ''),\n",
    "                                'price': result.get('units', [])[0].get('price', ''),\n",
    "                                'beds': result.get('units', [])[0].get('beds', ''),\n",
    "                                'url': result.get('detailUrl', '')\n",
    "                            }\n",
    "                            json_list.append(result_info)\n",
    "                    elif 'regionState' in page_props:\n",
    "                        # Extract region information\n",
    "                        region_info = page_props.get('regionState', {}).get('regionInfo', [])\n",
    "                        for region in region_info:\n",
    "                            region_info = {\n",
    "                                'regionId': region.get('regionId', ''),\n",
    "                                'regionType': region.get('regionType', ''),\n",
    "                                'regionName': region.get('regionName', '')\n",
    "                            }\n",
    "                            json_list.append(region_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    return json_list\n",
    "\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.html'):  # Process only HTML files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Call the function to parse HTML content\n",
    "        html_content = parse_html_file(file_path)\n",
    "        \n",
    "        # Call the function to extract home details\n",
    "        address_details_dict = extract_home_details(html_content)\n",
    "        \n",
    "        # Call the function to extract listing info\n",
    "        listing_info = extract_listing_info(html_content)\n",
    "        \n",
    "        # Aggregate data into the lists or dictionaries\n",
    "        all_address_details.update(address_details_dict)\n",
    "        all_listing_info.extend(listing_info)\n",
    "\n",
    "# Now, you can process or store the aggregated data as needed\n",
    "print(\"Aggregated Home Details:\", all_address_details)\n",
    "print(\"Aggregated Listing Info:\", all_listing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some listings the information on number of bedroom, number of bathroom, and apartment size is incomplete or mixed up. I performed data manipulation to fix the mistaken values and clean up the extra symbols such as comma and dollar sign. <br\\ >\n",
    "Finally, I have two data sets containing the housing information for apartments for rent and apartments for sale. My for sale data set has 8,456 rows and 8 columns, and the for rent data set has 20,988 rows and 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#is the apartment furnished?\n",
    "cond=data['bed']=='Furnished'\n",
    "data.loc[cond,'furnished']=1\n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from size to bath\n",
    "cond=[]\n",
    "for i in data['size']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'size'] \n",
    "data.loc[cond,'size']=''\n",
    "\n",
    "#move from bed to bath\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='Furnished' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from bath to bed\n",
    "cond=[]\n",
    "for i in data['bath']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(len(i.split(\" \"))==1):\n",
    "            cond.append(True)\n",
    "        else:\n",
    "            if(i.split(\" \")[1] in ('bath','baths')):\n",
    "                cond.append(False)\n",
    "            else:\n",
    "                cond.append(True)\n",
    "data.loc[cond,'bed']=data.loc[cond,'bath'] \n",
    "data.loc[cond,'bath']=''\n",
    "\n",
    "#move from bed to size\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(i.split(\" \")[1] in ('bed','beds')):\n",
    "            cond.append(False)\n",
    "        else:\n",
    "            cond.append(True)\n",
    "data.loc[cond,'size']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "\n",
    "#replace blank with nan\n",
    "data=data.applymap(lambda x: np.nan if x=='' else x)\n",
    "\n",
    "#data\n",
    "data.to_csv('rent.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size to numeric\n",
    "cond=data['size'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'size']=int(data['size'][i].split(\" \")[0].replace(',',''))\n",
    "#bath to numeric\n",
    "cond=data['bath'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bath']=float(data['bath'][i].split(\" \")[0].replace('+',''))\n",
    "#bed to numeric\n",
    "cond=data['bed'].isnull()\n",
    "data['bed']=data['bed'].replace('studio','0 bed')\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bed']=float(data['bed'][i].split(\" \")[0].replace(',','').replace('+',''))\n",
    "#remove dollar sign\n",
    "data['price']=[int(i.replace('$','').replace(',','')) for i in data['price']]\n",
    "\n",
    "data.to_csv('rent_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "book = 'zillow_sales_search_results.txt'\n",
    "#book = 'zillow_rental_search_results.txt'\n",
    "\n",
    "ZPID_Set = set()\n",
    "\n",
    "with open(book, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if '\"zpid\"' in line:  # Ensure that only lines with proper ZPID information are considered\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) >= 2: #This code snippet checks if the length of the list or string variable 'parts' is greater than or equal to 2. \n",
    "                ZPID = parts[1].strip().rstrip(',').replace('\"', '')  # Extracting the value of ZPID from the line\n",
    "                if ZPID not in ZPID_Set:  # Check if ZPID is already in the set\n",
    "                    ZPID_Set.add(ZPID)  # Add ZPID to the set\n",
    "\n",
    "print('Total Number of lines: ', len(lines))\n",
    "print('Total number of unique ZPID: ', len(ZPID_Set))\n",
    "print(ZPID_Set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one below works specifically for parsing local JSON local sales info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def parse_zillow_results_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    try:\n",
    "        zillow_data = json.loads(data)\n",
    "        list_results = zillow_data['searchResults']['listResults']\n",
    "        return list_results\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_zillow_properties(properties):\n",
    "    parsed_properties = []\n",
    "    for property_data in properties:\n",
    "        parsed_property = {\n",
    "            'ID': property_data.get('zpid', ''),\n",
    "            'Property Type': property_data.get('statusText', ''),\n",
    "            'Price': property_data.get('price', ''),\n",
    "            'Address': property_data.get('addressStreet', ''),\n",
    "            'City': property_data.get('addressCity', ''),\n",
    "            'State': property_data.get('addressState', ''),\n",
    "            'Zipcode': property_data.get('addressZipcode', ''),\n",
    "            'Beds': property_data.get('beds', 0),\n",
    "            'Baths': property_data.get('baths', 0),\n",
    "            'Area (sq. ft)': property_data.get('area', 0),\n",
    "            'Zestimate': property_data.get('hdpData', {}).get('homeInfo', {}).get('zestimate', 0),\n",
    "            'RentZestimate': property_data.get('hdpData', {}).get('homeInfo', {}).get('rentZestimate', 0),\n",
    "            'Days on Zillow': property_data.get('hdpData', {}).get('homeInfo', {}).get('daysOnZillow', 0),\n",
    "            'Website': property_data.get('detailUrl',0)\n",
    "        }\n",
    "        parsed_properties.append(parsed_property)\n",
    "    return parsed_properties\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'zillow_sales_search_results.txt'\n",
    "properties = parse_zillow_results_from_file(file_path)\n",
    "parsed_properties = parse_zillow_properties(properties)\n",
    "\n",
    "print('# Properties Parsed: ', len(parsed_properties))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(parsed_properties)\n",
    "\n",
    "# Convert numeric columns to integers\n",
    "numeric_columns = ['Beds', 'Baths', 'Area (sq. ft)', 'Zestimate', 'RentZestimate', 'Days on Zillow']\n",
    "for col in numeric_columns:\n",
    "    df[col] = df[col].astype(int) # Convert to float\n",
    "# Clean and convert 'price' column\n",
    "df['Price'] = df['Price'].str.replace('[\\$,]', '', regex=True)  # Remove '$' and ','\n",
    "df['Price'] = df['Price'].str.replace('K', '000').str.replace('M', '000000')  # Replace 'K' and 'M' with zeros\n",
    "df['Property Type'] = df['Property Type'].str.replace('for sale', '', regex=True)  # Remove '$' and ','\n",
    "\n",
    "# Count the number of unique ZPID values\n",
    "unique_zpids = df['ID'].nunique()\n",
    "print('Number of unique ZPIDs:', unique_zpids)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Convert to numeric with coercion to handle non-numeric values\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert non-numeric values to NaN\n",
    "\n",
    "# Drop non-numeric columns\n",
    "numeric_df = df[numeric_columns]\n",
    "\n",
    "#for col in df.columns:\n",
    "#    unique_values = df[col].unique()\n",
    "#    print(f\"Unique values in {col}: {unique_values}\")\n",
    "\n",
    "\n",
    "# Perform data analysis and visualization here\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "book = 'zillow_rental_search_p1.txt'\n",
    "#book = 'zillow_rental_search_results.txt'\n",
    "\n",
    "ZPID_Set = set()\n",
    "\n",
    "with open(book, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if '\"zpid\"' in line:  # Ensure that only lines with proper ZPID information are considered\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) >= 2: #This code snippet checks if the length of the list or string variable 'parts' is greater than or equal to 2. \n",
    "                ZPID = parts[1].strip().rstrip(',').replace('\"', '')  # Extracting the value of ZPID from the line\n",
    "                if ZPID not in ZPID_Set:  # Check if ZPID is already in the set\n",
    "                    ZPID_Set.add(ZPID)  # Add ZPID to the set\n",
    "\n",
    "print('Total Number of lines: ', len(lines))\n",
    "print('Total number of unique ZPID: ', len(ZPID_Set))\n",
    "print(ZPID_Set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address: 224 E 135th St, Bronx, NY\n",
      "Address: 751 E 89th St UNIT 10, Brooklyn, NY 11236\n",
      "Address: 234 Hull St #1B, Brooklyn, NY 11233\n",
      "Address: 2016 Albemarle Rd APT 2D, Brooklyn, NY 11226\n",
      "Address: 2272 1st Ave APT 1, New York, NY 10035\n",
      "Address: 150 W 140th St APT 4F, New York, NY 10030\n",
      "Address: 48 Saint Nicholas Pl #37, New York, NY 10031\n",
      "Address: 1092 Lenox Rd APT 2R, Brooklyn, NY 11212\n",
      "Address: 236 Howard Ave #6, Brooklyn, NY 11233\n",
      "Address: 180 Myrtle Ave, Brooklyn, NY\n",
      "Address: 43-12 Hunter St, Long Island City, NY\n",
      "Address: 236 E 47th St APT 20E, New York, NY 10017\n",
      "Address: 2206 78th St APT B1, Flushing, NY 11370\n",
      "Address: 698 10th Ave, New York, NY\n",
      "Address: 251 W 81st St #6Z, New York, NY 10024\n",
      "Address: 56 7th Ave, New York, NY\n",
      "Address: 313 E 60th St APT 2C, New York, NY 10022\n",
      "Address: 106 Washington Ave #2G, Brooklyn, NY 11205\n",
      "Address: (undisclosed Address), Bronx, NY 10453\n",
      "Address: (undisclosed Address), Bellerose, NY 11426\n",
      "Address: 30 Magaw Pl APT 5D, New York, NY 10033\n",
      "Address: 82-40 Austin St, Kew Gardens, NY\n",
      "Address: (undisclosed Address), Flushing, NY 11355\n",
      "Address: 1503 Atlantic Ave APT 3, Brooklyn, NY 11213\n",
      "Address: 6907 222nd St FLOOR 1, Oakland Gardens, NY 11364\n",
      "Address: 723 Saint Nicholas Ave #36, New York, NY 10031\n",
      "Address: 1306 Saint Johns Pl #3F, Brooklyn, NY 11213\n",
      "Address: 25-32 30th Dr #7T, Astoria, NY 11102\n",
      "Address: 363 Prospect Pl #1Q, Brooklyn, NY 11238\n",
      "Address: 279 E 44th St, New York, NY\n",
      "Address: 1697 Nostrand Ave, Brooklyn, NY\n",
      "Address: 223 E 96th St, New York, NY\n",
      "Address: 238 Central Ave APT 1B, Brooklyn, NY 11221\n",
      "Address: 219 78th St, Brooklyn, NY\n",
      "Address: 836 Flatbush Ave APT 11, Brooklyn, NY 11226\n",
      "Address: 193 E 7th St APT 1, New York, NY 10009\n",
      "Address: 431 W 37th St, New York, NY\n",
      "Address: 41 W 86th St, New York, NY\n",
      "Address: 400 W 37th St, New York, NY\n",
      "Address: 1128 Bushwick Ave #1A, Brooklyn, NY 11221\n",
      "Address: 100 W 138th St, New York, NY\n",
      "Address: 30-84 29th St #4C, Astoria, NY 11102\n",
      "Address: 1009 E 37th St #1R, Brooklyn, NY 11210\n",
      "Address: 23-20 30th Dr #2TT, Astoria, NY 11102\n",
      "Address: 4304 54th St FLOOR 3, Woodside, NY 11377\n",
      "Address: 863 Hart St APT 2R, Brooklyn, NY 11237\n",
      "Address: 321 Putnam Ave #1N, Brooklyn, NY 11216\n",
      "Address: 331 W 35th St #2B, New York, NY 10001\n",
      "Address: 43-08 52nd St, Woodside, NY\n",
      "Address: 13515 83rd Ave APT 4R, Jamaica, NY 11435\n",
      "Address: 362 E 46th St #1, Brooklyn, NY 11203\n",
      "Address: 171 E 101st St, New York, NY\n",
      "Address: 142 W 76th St APT 6, New York, NY 10023\n",
      "Address: 536 Kosciuszko St #3F, Brooklyn, NY 11221\n",
      "Address: 685 Pennsylvania Ave FLOOR 1, Brooklyn, NY 11207\n",
      "Address: 1065 Lafayette Ave, Brooklyn, NY\n",
      "Address: 331 W 43rd St APT 4B, New York, NY 10036\n",
      "Address: 1159 President St APT 1D, Brooklyn, NY 11225\n",
      "Address: 6264 Saunders St, Rego Park, NY\n",
      "Address: 502 62nd St #3V, Brooklyn, NY 11220\n",
      "Address: 3045 Godwin Ter #4W, Bronx, NY 10463\n",
      "Address: 153 Erasmus St, Brooklyn, NY\n",
      "Address: 420 Hancock St APT 3, Brooklyn, NY 11216\n",
      "Address: 23-68 24th St #2F, Astoria, NY 11105\n",
      "Address: 618 Crescent Ave APT 8, Bronx, NY 10458\n",
      "Address: 1625 Park Pl #8, Brooklyn, NY 11233\n",
      "Address: 257 Quincy St APT 1A, Brooklyn, NY 11216\n",
      "Address: 322 E 93rd St, New York, NY\n",
      "Address: 321 Schaefer St, Brooklyn, NY\n",
      "Address: 411 E 114th St APT 20, New York, NY 10029\n",
      "Address: 2225 Tilden Ave, Brooklyn, NY\n",
      "Address: 823 Madison St #7, Brooklyn, NY 11221\n",
      "Address: 358 Lincoln Rd #3G, Brooklyn, NY 11225\n",
      "Address: 98 Kingston Ave, Brooklyn, NY\n",
      "Address: 38 Cooper St, Brooklyn, NY\n",
      "Address: Avalon Morningside Park | 1 Morningside Dr, New York, NY\n",
      "Address: Columbus Square | 808 Columbus Ave, New York, NY\n",
      "Address: 10 Hanover Square | 10 Hanover Sq, New York, NY\n",
      "Address: The Ashley | 400 W 63rd St, New York, NY\n",
      "Address: Avalon Bowery Place | 11 E 1st St, New York, NY\n",
      "Address: Aalto57 | 1065 2nd Ave, New York, NY\n",
      "Address: Beach 101 | 101-19 Rockaway Beach Blvd, Rockaway Park, NY\n",
      "Address: 180 Montague | 180 Montague St, Brooklyn, NY\n",
      "Address: 15 Cliff | 15 Cliff St, New York, NY\n",
      "Address: 109-02 108th St FLOOR 2, South Ozone Park, NY 11420\n",
      "Address: 13828 Queens Blvd, Jamaica, NY\n",
      "Address: 16 Micieli Pl #2, Brooklyn, NY 11218\n",
      "Address: 85 John St APT 2G, New York, NY 10038\n",
      "Address: (undisclosed Address), Woodhaven, NY 11421\n",
      "Address: (undisclosed Address), Bronx, NY 10460\n",
      "Address: 7000 Bay Pkwy APT 4I, Brooklyn, NY 11204\n",
      "Address: (undisclosed Address), Bronx, NY 10461\n",
      "Address: 129 W 85th St #2, New York, NY 10024\n",
      "Address: 1630 Brooklyn Ave #2A, Brooklyn, NY 11210\n",
      "Address: 112 W 138th St APT 5A, New York, NY 10030\n",
      "Address: 1436 Lexington Ave APT 4D, New York, NY 10128\n",
      "Address: 321 E 83rd St, New York, NY 10028\n",
      "Address: 575 2nd Ave APT 2B, New York, NY 10016\n",
      "Address: 166 W 122nd St APT 5W, New York, NY 10027\n",
      "Address: 202 6th Ave APT 5B, New York, NY 10013\n",
      "Address: 1904 Nostrand Ave APT 2E, Brooklyn, NY 11226\n",
      "Address: 303 E 46th St APT 2W, New York, NY 10017\n",
      "Address: 110 Macdougal St, New York, NY\n",
      "Address: 447 W 37th St APT 3FW, New York, NY 10018\n",
      "Address: 10 Charles St #1, New York, NY 10014\n",
      "Address: 1620 New York Ave #3E, Brooklyn, NY 11210\n",
      "Address: 555 Waverly Ave #2E, Brooklyn, NY 11238\n",
      "Address: 7 Dey St, New York, NY\n",
      "Address: 773 E 32nd St #5, Brooklyn, NY 11210\n",
      "Address: The Tides At Arverne By The Sea | 190 Beach 69th St, Arverne, NY\n",
      "Address: 140-60 Beech LLC | 140-60 Beech Ave, Flushing, NY\n",
      "Address: Lincoln at Bankside | 5 Lincoln Ave, Bronx, NY\n",
      "Address: Park Haven Place | 88 153rd St, Jamaica, NY\n",
      "Address: Hillside Place | 8750 167th St, Jamaica, NY\n",
      "Address: The Capitol | 776 6th Ave, New York, NY\n",
      "Address: Estela Properties | 414-445 Gerard Ave, Bronx, NY\n",
      "Address: West Wharf | 60 Wharf Dr, Brooklyn, NY\n",
      "Address: The 88 | 15310 88th Ave, Jamaica, NY\n",
      "Address: 388 Bridge St, Brooklyn, NY\n",
      "Address: 54 Troutman St #4B, Brooklyn, NY 11206\n",
      "Address: 818 Lexington Ave, Brooklyn, NY\n",
      "Address: 46-02 70th St, Woodside, NY\n",
      "Address: 97-30 116th St #2, South Richmond Hill, NY 11419\n",
      "Address: 30 Charlton St APT 4C, New York, NY 10014\n",
      "Address: 167 Schenectady Ave, Brooklyn, NY 11213\n",
      "Address: 32-30 93rd St, East Elmhurst, NY\n",
      "Address: 460 W 23rd St APT 3A, New York, NY 10011\n",
      "Address: 417 E 57th St, New York, NY\n",
      "Address: 263 S 4th St, Brooklyn, NY\n",
      "Address: 426 Manhattan Ave, Brooklyn, NY\n",
      "Address: 133 N 4th St #4A, Brooklyn, NY 11249\n",
      "Address: 764 9th Ave #4, New York, NY 10019\n",
      "Address: 30 E 37th St APT 6B, New York, NY 10016\n",
      "Address: 184 E 89th St, Brooklyn, NY\n",
      "Address: 354A Quincy St #2A, Brooklyn, NY 11216\n",
      "Address: 65-23 Dieterle Cres FLOOR 2, Rego Park, NY 11374\n",
      "Address: 67-58 79th St #1, Middle Village, NY 11379\n",
      "Address: 509 W 159th St APT 16, New York, NY 10032\n",
      "Address: 12310 Ocean Promenade APT 6K, Far Rockaway, NY 11694\n",
      "Address: 87-01 Midland Pkwy #3N, Jamaica, NY 11432\n",
      "Address: 2118 3rd Ave #5, New York, NY 10029\n",
      "Address: 1337 Nostrand Ave #3A, Brooklyn, NY 11226\n",
      "Address: 395 Maple St APT A2, Brooklyn, NY 11225\n",
      "Address: 85 Poughkeepsie Ct #BASEMENT, Staten Island, NY 10312\n",
      "Address: 287 Kingston Ave #3R, Brooklyn, NY 11213\n",
      "Address: 360 S 1st St #35, Brooklyn, NY 11211\n",
      "Address: 340 E 93rd St APT 7D, New York, NY 10128\n",
      "Address: 231 Jefferson Ave #3RR, Brooklyn, NY 11216\n",
      "Address: 117 W 96th St, New York, NY\n",
      "Address: 214 W 92nd St APT 2A, New York, NY 10025\n",
      "Address: 114 E 40th St APT 8D, New York, NY 10016\n",
      "Address: 345 E 64th St, New York, NY\n",
      "Address: 928 Jefferson Ave #2, Brooklyn, NY 11221\n",
      "Address: (undisclosed Address), Staten Island, NY 10306\n",
      "Address: 580 Flatbush Ave APT 11J, Brooklyn, NY 11225\n",
      "Address: 320 Greene Ave APT 1, Brooklyn, NY 11238\n",
      "Address: 34-20 32nd St #8I, Astoria, NY 11106\n",
      "Address: 95 N 7th St #1C, Brooklyn, NY 11249\n",
      "Address: 110 W Houston St APT 1A, New York, NY 10012\n",
      "Address: 14415 41st Ave, Flushing, NY\n",
      "Address: 47 Catherine St APT 2B, New York, NY 10038\n",
      "Address: 959 E 78th St FLOOR 2, Brooklyn, NY 11236\n",
      "Address: 1002 Park Pl #2, Brooklyn, NY 11213\n",
      "Address: 45-10 Kissena Blvd #1H, Flushing, NY 11355\n",
      "Address: 240 Nagle Ave, New York, NY\n",
      "Address: 139-9 84th Dr #209, Jamaica, NY 11435\n",
      "Address: 1079 Lafayette Ave, Brooklyn, NY\n",
      "Address: 45 161st St FLOOR 2, Flushing, NY 11358\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 114\u001b[0m\n\u001b[1;32m    108\u001b[0m         addressData \u001b[38;5;241m=\u001b[39m extract_home_and_listing_info(html_content, page_name)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Now, you have a single dictionary containing combined home details and listing information for each address\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#print(\"Combined Data:\", len(listingData), listingData)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m         \n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Convert the list of dictionaries to a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdata\u001b[49m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the HTML files\n",
    "directory = 'Pretty_Resources/Zillow/Zillow Rentals/'\n",
    "\n",
    "# Initialize an empty dictionary to store aggregated data\n",
    "listingData = {}\n",
    "\n",
    "# Function to parse HTML content from a file\n",
    "def parse_html_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "    return html_content\n",
    "\n",
    "def extract_home_and_listing_info(html_content, page_name):\n",
    "    \"\"\"\n",
    "    Extracts unique home details and listing information corresponding to each address \n",
    "    from the provided HTML content.\n",
    "\n",
    "    Args:\n",
    "    - html_content (str): The HTML content to parse.\n",
    "    - page_name (str): The name of the page associated with the HTML content.\n",
    "\n",
    "    Returns:\n",
    "    - address_info_dict (dict): A dictionary where keys are addresses and values are dictionaries \n",
    "                                containing home details and listing information.\n",
    "    \"\"\"\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Initialize a dictionary to store address info\n",
    "    address_info_dict = {}\n",
    "    \n",
    "    # Find all list items within the main ul\n",
    "    list_items = soup.find_all('li', class_='ListItem-c11n-8-84-3__sc-10e22w8-0 StyledListCardWrapper-srp__sc-wtsrtn-0 iCyebE gTOWtl')\n",
    "    \n",
    "    # Iterate through the list items\n",
    "    for item in list_items:\n",
    "        # Extract address information\n",
    "        address_tag = item.find('a', class_='StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 jnnxAW property-card-link')\n",
    "        if address_tag:\n",
    "            address_info = address_tag.get_text(strip=True)\n",
    "            print(\"Address:\", address_info)\n",
    "        \n",
    "            # Find the ul containing home details within each list item\n",
    "            home_details_ul = item.find('ul', class_='StyledPropertyCardHomeDetailsList-c11n-8-84-3__sc-1xvdaej-0 eYPFID')\n",
    "            if home_details_ul:\n",
    "                # Initialize variables to store home details\n",
    "                beds = None\n",
    "                baths = None\n",
    "                sqft = None\n",
    "                price = None\n",
    "                # Extract the text content of each li within the ul\n",
    "                home_details_list = home_details_ul.find_all('li')\n",
    "                for detail in home_details_list:\n",
    "                    text = detail.get_text(strip=True)\n",
    "                    if 'bd' in text:\n",
    "                        beds = text\n",
    "                    elif 'ba' in text:\n",
    "                        baths = text\n",
    "                    elif 'sqft' in text:\n",
    "                        sqft = text\n",
    "                    elif '$' in text:\n",
    "                        price = text\n",
    "\n",
    "                # Find JSON script tags containing listing information\n",
    "                script_tags = item.find_all('script')\n",
    "                for script_tag in script_tags:\n",
    "                    script_content = script_tag.string\n",
    "                    if script_content:\n",
    "                        try:\n",
    "                            json_content = json.loads(script_content)\n",
    "                            if 'props' in json_content and 'pageProps' in json_content['props']:\n",
    "                                page_props = json_content['props']['pageProps']\n",
    "                                if 'searchResults' in page_props:\n",
    "                                    search_results = page_props['searchResults'].get('listResults', [])\n",
    "                                    for result in search_results:\n",
    "                                        listing_info = {\n",
    "                                            'address': address_info,\n",
    "                                            'zpid': result.get('zpid', ''),\n",
    "                                            'property_type': result.get('statusType', ''),\n",
    "                                            'price': result.get('units', [])[0].get('price', ''),\n",
    "                                            'beds': result.get('units', [])[0].get('beds', ''),\n",
    "                                            'baths': result.get('units', [])[0].get('baths', ''),\n",
    "                                            'sqft': result.get('units', [])[0].get('area', ''),\n",
    "                                            'url': result.get('detailUrl', ''),\n",
    "                                            'page': page_name\n",
    "                                        }\n",
    "                                        address_info_dict[address_info] = listing_info\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "    \n",
    "    return address_info_dict\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.html'):  # Process only HTML files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        page_name = os.path.splitext(filename)[0]  # Get the page name without the extension\n",
    "        \n",
    "        # Call the function to parse HTML content\n",
    "        html_content = parse_html_file(file_path)\n",
    "        \n",
    "        # Call the function to extract home details and listing info\n",
    "        addressData = extract_home_and_listing_info(html_content, page_name)\n",
    "        \n",
    "# Now, you have a single dictionary containing combined home details and listing information for each address\n",
    "#print(\"Combined Data:\", len(listingData), listingData)\n",
    "        \n",
    "# Convert the list of dictionaries to a Pandas DataFrame\n",
    "df = pd.DataFrame(addressData)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch_page_content(url_or_filepath):\n",
    "    # Check if the URL is a local file path\n",
    "    if os.path.exists(url_or_filepath):\n",
    "        # Open the local file and read its content\n",
    "        with open(url_or_filepath, 'r') as file:\n",
    "            content = file.read()\n",
    "    else:\n",
    "        # Fetch the page content using requests\n",
    "        response = requests.get(url_or_filepath)\n",
    "        content = response.content\n",
    "    return content\n",
    "\n",
    "def extract_rental_info(rental_element):\n",
    "    rental = {}\n",
    "\n",
    "    # Extract ZPID for each rental element\n",
    "    rental['zpid'] = rental_element.find('article')['id'].split('_')[-1]\n",
    "\n",
    "    # Extract Event Information\n",
    "    event_script = rental_element.find('script', type='application/ld+json')\n",
    "    if event_script:\n",
    "        event_data = json.loads(event_script.text)\n",
    "        event_location = event_data.get('location', [{}])[1].get('address', {})\n",
    "\n",
    "        rental.update({\n",
    "            'event_image': event_data.get('image', ''),\n",
    "            'event_start_date': event_data.get('startDate', ''),\n",
    "            'event_end_date': event_data.get('endDate', ''),\n",
    "            'event_street_address': event_location.get('streetAddress', ''),\n",
    "            'event_postal_code': event_location.get('postalCode', ''),\n",
    "            'event_address_locality': event_location.get('addressLocality', ''),\n",
    "            'event_address_region': event_location.get('addressRegion', '')\n",
    "        })\n",
    "\n",
    "    # Extract Property Details\n",
    "    property_address_element = rental_element.find('address', {'data-test': 'property-card-addr'})\n",
    "    rental['property_address'] = property_address_element.text.strip() if property_address_element else ''\n",
    "\n",
    "    # Extract Prices\n",
    "    price_elements = rental_element.find_all('span', {'class': 'PropertyCardWrapper__StyledPriceLine-srp__sc-16e8gqd-1.iMKTKr'})\n",
    "    rental['prices'] = [elem.text.strip() for elem in price_elements] if price_elements else []\n",
    "\n",
    "    # Extract Property Web URL\n",
    "    property_web_url_element = rental_element.find('a', {'data-test': 'property-card-link'})\n",
    "    rental['property_web_url'] = property_web_url_element['href'] if property_web_url_element else ''\n",
    "\n",
    "    return rental\n",
    "\n",
    "def parse_rentals_page(url_or_filepath):\n",
    "    content = fetch_page_content(url_or_filepath)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    rentals = []\n",
    "    rental_elements = soup.find_all('div', {'data-renderstrat': 'ssr'})\n",
    "\n",
    "    for rental_element in rental_elements:\n",
    "        rental_info = extract_rental_info(rental_element)\n",
    "        rentals.append(rental_info)\n",
    "\n",
    "    return rentals\n",
    "\n",
    "def extract_sales_info(sales_element):\n",
    "    sale = {}\n",
    "\n",
    "    # Extract ZPID for each sales element\n",
    "    sale['zpid'] = sales_element.find('article')['id'].split('_')[-1]\n",
    "\n",
    "    # Extract Property Details\n",
    "    property_address_element = sales_element.find('address', {'data-address-label': 'property-address'})\n",
    "    sale['property_address'] = property_address_element.text.strip() if property_address_element else ''\n",
    "\n",
    "    # Extract Prices\n",
    "    price_element = sales_element.find('span', {'class': 'Text-c11n-8-84-3__aiai24-0 iZVatD'})\n",
    "    sale['price'] = price_element.text.strip() if price_element else ''\n",
    "\n",
    "    # Extract Property Web URL\n",
    "    property_web_url_element = sales_element.find('a', {'class': 'Anchor-c11n-8-84-3__sc-hn4bge-0 sc-ckEbSK kxrUt bnQEvw'})\n",
    "    sale['property_web_url'] = property_web_url_element['href'] if property_web_url_element else ''\n",
    "\n",
    "    return sale\n",
    "\n",
    "def parse_sales_page(url_or_filepath):\n",
    "    content = fetch_page_content(url_or_filepath)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    sales = []\n",
    "    sales_elements = soup.find_all('div', {'data-testid': 'home-card-sale'})\n",
    "\n",
    "    for sales_element in sales_elements:\n",
    "        sale_info = extract_sales_info(sales_element)\n",
    "        sales.append(sale_info)\n",
    "\n",
    "    return sales\n",
    "\n",
    "def fetch_and_parse_data(base_url, parse_function, pages_to_fetch, file_path=None):\n",
    "    all_data = []\n",
    "    for page_number in range(1, pages_to_fetch + 1):\n",
    "        if file_path:\n",
    "            url_or_filepath = file_path\n",
    "        else:\n",
    "            url_or_filepath = f\"{base_url}{page_number}_p/\"\n",
    "        data = parse_function(url_or_filepath)\n",
    "        all_data.extend(data)\n",
    "    return all_data\n",
    "\n",
    "def parse_pagination_number(url):\n",
    "    content = fetch_page_content(url)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    pagination_elements = soup.find_all('a', {'class': 'StyledPaginationButton-c11n-8-84-3__aiai24-1'})\n",
    "    if pagination_elements:\n",
    "        last_page_number = max(int(page.text) for page in pagination_elements if page.text.isdigit())\n",
    "        return last_page_number\n",
    "    return 0  \n",
    "\n",
    "def main():\n",
    "    # Choose between online source or local file\n",
    "    source_choice = input(\"Enter 'online' to use a website or 'local file' to use a file: \").lower()\n",
    "    if source_choice not in ['online', 'local file']:\n",
    "        print(\"Invalid choice.\")\n",
    "        return\n",
    "\n",
    "    file_path = None  # Initialize file_path outside of the if block\n",
    "\n",
    "    if source_choice == 'online':\n",
    "        # Choose between rentals or sales\n",
    "        choice = input(\"Enter 'rentals' or 'sales' to fetch data: \").lower()\n",
    "        if choice not in ['rentals', 'sales']:\n",
    "            print(\"Invalid choice.\")\n",
    "            return\n",
    "        # Fetch the URL based on the choice\n",
    "        if choice == 'rentals':\n",
    "            base_url = \"https://www.zillow.com/new-york-ny/rentals/\"\n",
    "        elif choice == 'sales':\n",
    "            base_url = \"https://www.zillow.com/new-york-ny/sales/\"\n",
    "        else:\n",
    "            print(\"Invalid choice.\")\n",
    "            return\n",
    "        # Get the number of pages to fetch\n",
    "        pages_to_fetch = parse_pagination_number(base_url)\n",
    "    else:  # Source choice is local file\n",
    "        file_path = input(\"Enter the local file path: \")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"File not found.\")\n",
    "            return\n",
    "        # No need for pagination logic for local files\n",
    "        pages_to_fetch = 1\n",
    "\n",
    "    # Prompt for the pages to scrape\n",
    "    pages_input = input(\"Enter 'all' to scrape all pages, 'range' to specify a range, or 'specific' for a specific page: \")\n",
    "    if pages_input.lower() == 'all':\n",
    "        pages_to_scrape = 'all'\n",
    "    elif pages_input.lower() == 'range':\n",
    "        start_page = int(input(\"Enter the start page: \"))\n",
    "        end_page = int(input(\"Enter the end page: \"))\n",
    "        pages_to_scrape = (start_page, end_page)\n",
    "    elif pages_input.lower() == 'specific':\n",
    "        specific_page = int(input(\"Enter the specific page: \"))\n",
    "        pages_to_scrape = (specific_page,)\n",
    "    else:\n",
    "        print(\"Invalid input for pages.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Pages to Scrape:\", pages_to_scrape)\n",
    "\n",
    "\n",
    "    # Fetch and parse the data\n",
    "    if choice == 'rentals':\n",
    "        parse_function = parse_rentals_page\n",
    "    elif choice == 'sales':\n",
    "        parse_function = parse_sales_page\n",
    "\n",
    "    all_data = fetch_and_parse_data(base_url, parse_function, pages_to_fetch, file_path)\n",
    "    all_data_df = pd.DataFrame(all_data)\n",
    "    if all_data_df.empty:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "    unique_zpids_count = all_data_df['zpid'].nunique()\n",
    "    \n",
    "\n",
    "    print(f\"\\n{choice.capitalize()} DataFrame:\")\n",
    "    print(all_data_df)\n",
    "    print(f\"\\nNumber of Unique {choice.capitalize()} ZPIDs:\", unique_zpids_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
