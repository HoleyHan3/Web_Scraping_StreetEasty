{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Zillow.com to analyze housing price in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal here is to collect housing prices for both rental and sale in New York city. I looked at three major real estate website including Trulia, Zillow, and StreetEasy. Comparing to the other two websites, StreetEasy gives the most information on the searching results page and the format of each listing is very consistent, which is great for the purpose of web-scraping.<br\\ >\n",
    "<a href=\"http://zillow.com/\">\n",
    "<img \"StreetEasy\" src=\"map/streetEasy_logo.jpg\" height=\"30px\" width=\"150px\"/></a><br\\ >\n",
    "\n",
    "Web scraping is done using the beautifulsoup package in Python. I created two functions that can loop through all the pages of searching results, and also empty strings to store results. Below are the steps I took to scrape StreetEasy:\n",
    "1. Analyzing the HTML page: HTML code of a web page can be viewed by right click and selecting 'Inspect'. This helps us identifying the HTML tags of the information to be scraped\n",
    "2. Making the soup!: It is important to select the correct parser for your data type. I used HTML parser.\n",
    "3. Navigating the parse tree and iterate through tags: once the soup is made, we have the HTML code in Python. We can then find our desired information by searching through HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def package_url_sale(page):\n",
    "    \"\"\"Creates a URL for sales based on the page number.\"\"\"\n",
    "    return f'https://www.zillow.com/new-york-ny/for_sale/{page}_p/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def package_url_rent(page):\n",
    "    \"\"\"Creates a URL for rentals based on the page number.\"\"\"\n",
    "    return f'https://www.zillow.com/new-york-ny/rentals/{page}_p/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "#price=[]\n",
    "#where=[]\n",
    "#bed=[]\n",
    "#bath=[]\n",
    "#size=[]\n",
    "#monthly=[]\n",
    "#street=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "# Specify the path to chromedriver\n",
    "#chromedriver_path = './drivers/chromedriver-mac-arm64/'\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "def make_request(session, url):\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        return response.content  # or response.text, depending on what you need\n",
    "    except requests.Timeout:\n",
    "        print(\"Request timed out.\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "    return None\n",
    "\n",
    "def scrape_data() -> None:\n",
    "    \"\"\"Scrapes data from StreetEasy and populates price, where, bed, bath, size, and street lists.\"\"\"\n",
    "    price: List[str] = []\n",
    "    where: List[str] = []\n",
    "    bed: List[str] = []\n",
    "    bath: List[str] = []\n",
    "    size: List[str] = []\n",
    "    street: List[str] = []\n",
    "\n",
    "    # Define the total number of pages\n",
    "    total_pages = 2\n",
    "\n",
    "    # Create a DataFrame outside the loop to avoid creating it in every iteration\n",
    "    data = {'street': street, 'price': price, 'where': where, 'bed': bed, 'bath': bath, 'size': size}\n",
    "\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    # If you want to run Chrome in headless mode, uncomment the next line\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    # Create a tqdm progress bar for the loop\n",
    "    progress_bar = tqdm(range(1, total_pages + 1), desc=\"Scraping Pages\", position=0)\n",
    "\n",
    "    # Set up the Selenium webdriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    for x in progress_bar:\n",
    "        url = package_url_rent(str(x))\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "\n",
    "        # Use Selenium to fetch the page content\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load (adjust the timeout as needed)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'property-card-data'))\n",
    "        )\n",
    "\n",
    "    # Simulate scrolling for more interaction\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_by_offset(0, 500).perform()\n",
    "\n",
    "        # Add a random delay (between 5 and 15 seconds)\n",
    "        time.sleep(random.uniform(5, 15))\n",
    "\n",
    "        # Continue with scraping using the response content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Get the page source\n",
    "        r = driver.page_source\n",
    "\n",
    "        if r:\n",
    "            print(f\"Response Content: {r}\")\n",
    "        else:\n",
    "            print(\"Request failed or timed out.\")\n",
    "\n",
    "        time.sleep(10)  # Implement a rate limit (e.g., 1 request per 2 seconds)\n",
    "\n",
    "        # Continue with scraping using the response content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the script tag with type=\"application/ld+json\"\n",
    "        script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "        listings = []\n",
    "        # Inside the loop where you are scraping the page content\n",
    "        for listing in soup.find_all('div', {'class': 'property-card-data'}):\n",
    "            result = {}\n",
    "\n",
    "            # Extract information from the script tag\n",
    "            script_tag = listing.find_previous('script', {'type': 'application/ld+json'})\n",
    "\n",
    "            try:\n",
    "                if script_tag:\n",
    "                    script_content = script_tag.string\n",
    "                    json_data = json.loads(script_content)\n",
    "\n",
    "                    # Extract information from the JSON data\n",
    "                    result['zpid'] = json_data.get('url', '').split('/')[-2]\n",
    "                    result['property_type']=json_data.get('type','')\n",
    "                    result['address'] = json_data.get('name', '')\n",
    "                    result['floor_size'] = json_data.get('floorSize', {}).get('value', '')\n",
    "                    result['street_address'] = json_data.get('address', {}).get('streetAddress', '')\n",
    "                    result['locality'] = json_data.get('address', {}).get('addressLocality', '')\n",
    "                    result['region'] = json_data.get('address', {}).get('addressRegion', '')\n",
    "                    result['postal_code'] = json_data.get('address', {}).get('postalCode', '')\n",
    "                    result['latitude'] = json_data.get('geo', {}).get('latitude', '')\n",
    "                    result['longitude'] = json_data.get('geo', {}).get('longitude', '')\n",
    "                    result['url'] = json_data.get('url', '')\n",
    "\n",
    "                    # Add the result to the listings\n",
    "                    listings.append(result)\n",
    "                    continue  # Skip the property card extraction if JSON extraction succeeds\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decoding failed. Using property card information. Error: {e}\")\n",
    "            \n",
    "                # Extract information from the property card\n",
    "            result['zpid'] = listing.find('address', {'data-test': 'property-card-addr'}).get_text().strip()\n",
    "            result['address'] = result.get('address', '')  # Don't overwrite if JSON extraction succeeded\n",
    "            result['floor_size'] = ''  # You may modify this based on the property card structure\n",
    "            result['street_address'] = ''  # You may modify this based on the property card structure\n",
    "            result['locality'] = ''  # You may modify this based on the property card structure\n",
    "            result['region'] = ''  # You may modify this based on the property card structure\n",
    "            result['postal_code'] = ''  # You may modify this based on the property card structure\n",
    "            result['latitude'] = ''  # You may modify this based on the property card structure\n",
    "            result['longitude'] = ''  # You may modify this based on the property card structure\n",
    "            result['url'] = ''  # You may modify this based on the property card structure\n",
    "\n",
    "            # Continue extracting information from the property card\n",
    "            result['price'] = listing.find('span', {'data-test': 'property-card-price'}).get_text().strip()\n",
    "            details_list = listing.find('ul', {'class': 'dmDolk'})\n",
    "            details = details_list.find_all('li') if details_list else []\n",
    "            result['bedrooms'] = details[0].get_text().strip() if len(details) > 0 else ''\n",
    "            result['bathrooms'] = details[1].get_text().strip() if len(details) > 1 else ''\n",
    "            result['sqft'] = details[2].get_text().strip() if len(details) > 2 else ''\n",
    "            type_div = listing.find('div', {'class': 'gxlfal'})\n",
    "            result['type'] = type_div.get_text().split(\"-\")[1].strip() if type_div else ''\n",
    "\n",
    "            # Add the result to the listings\n",
    "            data['zpid'].append(result['zpid'])\n",
    "            data['address'].append(result['address'])\n",
    "            data['floor_size'].append(result['floor_size'])\n",
    "            data['street_address'].append(result['street_address'])\n",
    "            data['locality'].append(result['locality'])\n",
    "            data['region'].append(result['region'])\n",
    "            data['postal_code'].append(result['postal_code'])\n",
    "            data['latitude'].append(result['latitude'])\n",
    "            data['longitude'].append(result['longitude'])\n",
    "            data['url'].append(result['url'])\n",
    "\n",
    "            # Add the result to the listings\n",
    "            data['price'].append(result['price'])\n",
    "            data['bed'].append(result['bedrooms'])\n",
    "            data['bath'].append(result['bathrooms'])\n",
    "            data['type'].append(result['type'])\n",
    "\n",
    "        # Increment the page number\n",
    "        # Update the progress bar description with the current page number\n",
    "        progress_bar.set_postfix(Page=f\"{x}/{total_pages}\")\n",
    "        progress_bar.refresh()  # Manually refresh the tqdm bar\n",
    "\n",
    "        time.sleep(12)  # Implement a rate limit (e.g., 1 request per 2 seconds)\n",
    "\n",
    "    progress_bar.close()  # Close the progress bar after processing all pages\n",
    "\n",
    "    # Close the Selenium webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Create DataFrame after the loop\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"done\")  # Print \"done\" outside the loop\n",
    "    return df\n",
    "\n",
    "# Call the scraping function\n",
    "scrape_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Example: Call the scraping function with 2 pages\u001b[39;00m\n\u001b[1;32m    158\u001b[0m total_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 159\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscrape_data_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_pages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Display the result DataFrame\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_df)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def make_request(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.read()\n",
    "    except aiohttp.ClientTimeout:\n",
    "        print(\"Request timed out.\")\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "    return None\n",
    "\n",
    "async def fetch_page_content(url, session):\n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    response_content = await make_request(session, url)\n",
    "    if response_content:\n",
    "        print(f\"Response Content: {response_content[:200]}...\")  # Display part of the content\n",
    "    else:\n",
    "        print(\"Request failed or timed out.\")\n",
    "    return response_content\n",
    "\n",
    "async def scrape_page(url, session, driver, data):\n",
    "    response_content = await fetch_page_content(url, session)\n",
    "\n",
    "    if response_content:\n",
    "        soup = BeautifulSoup(response_content, 'html.parser')\n",
    "\n",
    "        # Find the script tag with type=\"application/ld+json\"\n",
    "        script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "        listings = []\n",
    "\n",
    "        try:\n",
    "            if script_tag:\n",
    "                script_content = script_tag.string\n",
    "                json_data = json.loads(script_content)\n",
    "\n",
    "                # Extract information from the JSON data\n",
    "                result = {}\n",
    "                result['zpid'] = json_data.get('url', '').split('/')[-2]\n",
    "                result['property_type'] = json_data.get('type', '')\n",
    "                result['address'] = json_data.get('name', '')\n",
    "                result['floor_size'] = json_data.get('floorSize', {}).get('value', '')\n",
    "                result['street_address'] = json_data.get('address', {}).get('streetAddress', '')\n",
    "                result['locality'] = json_data.get('address', {}).get('addressLocality', '')\n",
    "                result['region'] = json_data.get('address', {}).get('addressRegion', '')\n",
    "                result['postal_code'] = json_data.get('address', {}).get('postalCode', '')\n",
    "                result['latitude'] = json_data.get('geo', {}).get('latitude', '')\n",
    "                result['longitude'] = json_data.get('geo', {}).get('longitude', '')\n",
    "                result['url'] = json_data.get('url', '')\n",
    "\n",
    "                # Add the result to the listings\n",
    "                listings.append(result)\n",
    "            else:\n",
    "                print(\"No JSON data found. Using property card information.\")\n",
    "\n",
    "            # Continue with scraping using the response content\n",
    "            for listing in soup.find_all('div', {'class': 'property-card-data'}):\n",
    "                result = {}\n",
    "\n",
    "                # Extract information from the property card\n",
    "                result['zpid'] = listing.find('address', {'data-test': 'property-card-addr'}).get_text().strip()\n",
    "                result['address'] = result.get('address', '')  # Don't overwrite if JSON extraction succeeded\n",
    "                result['floor_size'] = ''  # You may modify this based on the property card structure\n",
    "                result['street_address'] = ''  # You may modify this based on the property card structure\n",
    "                result['locality'] = ''  # You may modify this based on the property card structure\n",
    "                result['region'] = ''  # You may modify this based on the property card structure\n",
    "                result['postal_code'] = ''  # You may modify this based on the property card structure\n",
    "                result['latitude'] = ''  # You may modify this based on the property card structure\n",
    "                result['longitude'] = ''  # You may modify this based on the property card structure\n",
    "                result['url'] = ''  # You may modify this based on the property card structure\n",
    "\n",
    "                # Continue extracting information from the property card\n",
    "                result['price'] = listing.find('span', {'data-test': 'property-card-price'}).get_text().strip()\n",
    "                details_list = listing.find('ul', {'class': 'dmDolk'})\n",
    "                details = details_list.find_all('li') if details_list else []\n",
    "                result['bedrooms'] = details[0].get_text().strip() if len(details) > 0 else ''\n",
    "                result['bathrooms'] = details[1].get_text().strip() if len(details) > 1 else ''\n",
    "                result['sqft'] = details[2].get_text().strip() if len(details) > 2 else ''\n",
    "                type_div = listing.find('div', {'class': 'gxlfal'})\n",
    "                result['type'] = type_div.get_text().split(\"-\")[1].strip() if type_div else ''\n",
    "\n",
    "                # Add the result to the listings\n",
    "                listings.append(result)\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decoding failed. Error: {e}\")\n",
    "\n",
    "    # Increment the page number\n",
    "    print(\"Scraping done for:\", url)\n",
    "    return listings\n",
    "\n",
    "async def scrape_data_async(total_pages):\n",
    "    ua = UserAgent()\n",
    "    user_agent = ua.random\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    # Set up the Selenium webdriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    data = {\n",
    "        'zpid': [],\n",
    "        'address': [],\n",
    "        'floor_size': [],\n",
    "        'street_address': [],\n",
    "        'locality': [],\n",
    "        'region': [],\n",
    "        'postal_code': [],\n",
    "        'latitude': [],\n",
    "        'longitude': [],\n",
    "        'url': [],\n",
    "        'price': [],\n",
    "        'bed': [],\n",
    "        'bath': [],\n",
    "        'type': [],\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for x in range(1, total_pages + 1):\n",
    "            url = package_url_rent(str(x))\n",
    "            tasks.append(scrape_page(url, session, driver, data))\n",
    "\n",
    "        # Use asyncio.gather() to run tasks concurrently\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    listings = [item for sublist in results for item in sublist]\n",
    "\n",
    "    # Close the Selenium webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(listings)\n",
    "    return df\n",
    "\n",
    "# Example: Call the scraping function with 2 pages\n",
    "total_pages = 2\n",
    "result_df = asyncio.run(scrape_data_async(total_pages))\n",
    "\n",
    "# Display the result DataFrame\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some listings the information on number of bedroom, number of bathroom, and apartment size is incomplete or mixed up. I performed data manipulation to fix the mistaken values and clean up the extra symbols such as comma and dollar sign. <br\\ >\n",
    "Finally, I have two data sets containing the housing information for apartments for rent and apartments for sale. My for sale data set has 8,456 rows and 8 columns, and the for rent data set has 20,988 rows and 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#is the apartment furnished?\n",
    "cond=data['bed']=='Furnished'\n",
    "data.loc[cond,'furnished']=1\n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from size to bath\n",
    "cond=[]\n",
    "for i in data['size']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'size'] \n",
    "data.loc[cond,'size']=''\n",
    "\n",
    "#move from bed to bath\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='Furnished' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from bath to bed\n",
    "cond=[]\n",
    "for i in data['bath']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(len(i.split(\" \"))==1):\n",
    "            cond.append(True)\n",
    "        else:\n",
    "            if(i.split(\" \")[1] in ('bath','baths')):\n",
    "                cond.append(False)\n",
    "            else:\n",
    "                cond.append(True)\n",
    "data.loc[cond,'bed']=data.loc[cond,'bath'] \n",
    "data.loc[cond,'bath']=''\n",
    "\n",
    "#move from bed to size\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(i.split(\" \")[1] in ('bed','beds')):\n",
    "            cond.append(False)\n",
    "        else:\n",
    "            cond.append(True)\n",
    "data.loc[cond,'size']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "\n",
    "#replace blank with nan\n",
    "data=data.applymap(lambda x: np.nan if x=='' else x)\n",
    "\n",
    "#data\n",
    "data.to_csv('rent.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size to numeric\n",
    "cond=data['size'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'size']=int(data['size'][i].split(\" \")[0].replace(',',''))\n",
    "#bath to numeric\n",
    "cond=data['bath'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bath']=float(data['bath'][i].split(\" \")[0].replace('+',''))\n",
    "#bed to numeric\n",
    "cond=data['bed'].isnull()\n",
    "data['bed']=data['bed'].replace('studio','0 bed')\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bed']=float(data['bed'][i].split(\" \")[0].replace(',','').replace('+',''))\n",
    "#remove dollar sign\n",
    "data['price']=[int(i.replace('$','').replace(',','')) for i in data['price']]\n",
    "\n",
    "data.to_csv('rent_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
