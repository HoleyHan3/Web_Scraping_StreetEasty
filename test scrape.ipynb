{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Zillow.com to analyze housing price in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal here is to collect housing prices for both rental and sale in New York city. I looked at three major real estate website including Trulia, Zillow, and StreetEasy. Comparing to the other two websites, StreetEasy gives the most information on the searching results page and the format of each listing is very consistent, which is great for the purpose of web-scraping.<br\\ >\n",
    "<a href=\"http://zillow.com/\">\n",
    "<img \"StreetEasy\" src=\"map/streetEasy_logo.jpg\" height=\"30px\" width=\"150px\"/></a><br\\ >\n",
    "\n",
    "Web scraping is done using the beautifulsoup package in Python. I created two functions that can loop through all the pages of searching results, and also empty strings to store results. Below are the steps I took to scrape StreetEasy:\n",
    "1. Analyzing the HTML page: HTML code of a web page can be viewed by right click and selecting 'Inspect'. This helps us identifying the HTML tags of the information to be scraped\n",
    "2. Making the soup!: It is important to select the correct parser for your data type. I used HTML parser.\n",
    "3. Navigating the parse tree and iterate through tags: once the soup is made, we have the HTML code in Python. We can then find our desired information by searching through HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import lxml\n",
    "import numbers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req_headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-encoding': 'gzip, deflate, br',\n",
    "    'Accept-language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-insecure-requests': '1',\n",
    "    'User-agent': UserAgent().random,\n",
    "}\n",
    "\n",
    "base_url = \"https://streeteasy.com/for-sale/\"\n",
    "#base_url = \"https://www.zillow.com/homes/for_sale/\"\n",
    "urls = []\n",
    "\n",
    "city = 'nyc'\n",
    "url1 = base_url +city+'/'\n",
    "urls.append(url1)\n",
    "\n",
    "start_page = 2\n",
    "end_page = 3\n",
    "\n",
    "# Add all pages\n",
    "for i in range(start_page, end_page + 1):\n",
    "    dom = base_url + city + '/' + 'page_' + str(i) #streeteasy\n",
    "    #dom = base_url + city + '/' + str(i) '_p' +'/' #zillow\n",
    "    if dom not in urls:\n",
    "        urls.append(dom)\n",
    "\n",
    "print(urls)\n",
    "\n",
    "#https://streeteasy.com/for-sale/nyc/price:-400000?page=2 with max\n",
    "#https://streeteasy.com/for-sale/nyc/price:100000-700000 min and max\n",
    "#https://streeteasy.com/for-rent/nyc/price:-12500 for rent\n",
    "#https://streeteasy.com/for-rent/nyc/page_2 for rent\n",
    "\n",
    "# Define the rate limit: e.g., 5 calls per 60 seconds\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=60)\n",
    "def soups(data):\n",
    "    with requests.Session() as s:\n",
    "        r = s.get(data, headers=req_headers)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        #print(soup.prettify())  # Corrected line: Print prettified HTML content from the soup object\n",
    "    return soup\n",
    "\n",
    "\n",
    "#for url in urls:\n",
    "#    htmls=soups(url)\n",
    "# Call soup function and store output in a list\n",
    "#lst = []\n",
    "\n",
    "#for url in urls:\n",
    "#    htmls = soups(url)\n",
    "\n",
    "#print(len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches and saves prettify data from rental site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=60)\n",
    "def get_page_content(url,headers):\n",
    "    \"\"\"\n",
    "    Function to get the HTML content of a webpage given its URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page content: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_content_to_file(content, filename):\n",
    "    \"\"\"\n",
    "    Function to write content to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def main():\n",
    "    # Define the base URL and the number of pages you want to scrape\n",
    "    #base_url = \"https://zillow.com/new-york-ny/rentals\"\n",
    "    #base_url = \"https://zillow.com/new-york-ny/sales\"\n",
    "    #base_url = \"https://zillow.com/new-york-ny/sold\"\n",
    "    #base_url = \"http://streeteasy.com/for-sale/nyc?\" #1272 \n",
    "    base_url = \"http://streeteasy.com/for-rent/nyc?\" #1392\n",
    "    num_pages = 1392  # Number of additional pages to scrape\n",
    "    start_page = 1  # Starting page number\n",
    "    headers = {\n",
    "        'User-Agent': UserAgent().random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://google.com',\n",
    "        'Pragma': 'no-cache',\n",
    "        'Cache-Control': 'no-cache'\n",
    "    }\n",
    "\n",
    "    # Request and parse content for each page\n",
    "    for page_number in range(start_page, num_pages + 1):\n",
    "        #full_url = f\"{base_url}/{page_number}_p/\" Zillow        \n",
    "        full_url = f\"{base_url}page={page_number}\" # Streeteasy\n",
    "        print(f\"Fetching content from: {full_url}\")\n",
    "        page_content = get_page_content(full_url, headers)\n",
    "        if page_content:\n",
    "            # Write content to a file\n",
    "            #filename = f\"Page_{page_number}.html\"  # Create a filename based on the page number\n",
    "            #print(f\"Page {page_number} content fetched successfully.\")\n",
    "           # write_content_to_file(page_content, filename)\n",
    "           # print(f\"Prettified content written to: {filename}\")\n",
    "\n",
    "             # Parse content using BeautifulSoup and prettify\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            prettified_content = soup.prettify()\n",
    "            \n",
    "            # Write content to a file\n",
    "            prettyFilename = f\"pretty_page_{page_number}.html\"  # Create a filename based on the page number\n",
    "            write_content_to_file(prettified_content, prettyFilename)\n",
    "            print(f\"Prettified content written to: {prettyFilename}\")\n",
    "        else:\n",
    "            print(f\"No content found for: {full_url}\")\n",
    "        print(\"=\" * 50)  # Separator for better readability\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "rate_limit = 5 #calls/pages per minute\n",
    "period = 60 #a period of 60 seconds\n",
    "\n",
    "def estimate_time(rate_limit, period):\n",
    "    time_needed = pages / rate_limit #pages/5calls/minute\n",
    "    minutes = math.floor(time_needed)\n",
    "    seconds = math.ceil((time_needed - minutes) * 60)\n",
    "    return minutes, seconds\n",
    "\n",
    "# Example usage:\n",
    "pages = 98\n",
    "\n",
    "minutes, seconds = estimate_time(pages, rate_limit)\n",
    "print(f\"Estimated time needed: {minutes} minutes and {seconds} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "\n",
    "# Web URL\n",
    "web_url = \"https://www.zillow.com/new-york-ny/?searchQueryState=%7B%22pagination%22%3A%7B%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22west%22%3A-76.09592245507815%2C%22east%22%3A-71.8634395449219%2C%22south%22%3A40.120647691241565%2C%22north%22%3A41.27009728054556%7D%2C%22usersSearchTerm%22%3A%22New%20York%2C%20NY%22%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A6181%2C%22regionType%22%3A6%7D%5D%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22globalrelevanceex%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A9%7D\"\n",
    "# https://www.zillow.com/new-york-ny/for_sale/3_p/\n",
    "# Define request headers\n",
    "req_headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-encoding': 'gzip, deflate, br',\n",
    "    'Accept-language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-insecure-requests': '1',\n",
    "    'User-agent': UserAgent().random,\n",
    "}\n",
    "\n",
    "\n",
    "# Get URL Content\n",
    "r = requests.get(web_url, headers=req_headers)\n",
    "\n",
    "# Parse HTML Code\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "all_data = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
    "\n",
    "data = []\n",
    "for ld_json in all_data:\n",
    "    jsn = json.loads(ld_json.string)\n",
    "    data.append(jsn)\n",
    "    print(jsn)\n",
    "\n",
    "response_content = fetch_zillow_data(url)\n",
    "if response_content:\n",
    "    properties = extract_json_data_property_info(response_content)\n",
    "    for property in properties:\n",
    "        print(property)\n",
    "\n",
    "def extract_json_data_property_info(response_content):\n",
    "    \"\"\"\n",
    "    Extracts property information from Zillow data.\n",
    "    \n",
    "    Parameters:\n",
    "    - response_content (str): The content of the HTTP response containing Zillow data.\n",
    "    \n",
    "    Returns:\n",
    "    - property_info (list): A list containing dictionaries with property information.\n",
    "    \"\"\"\n",
    "    if response_content is None:\n",
    "        return []\n",
    "\n",
    "    property_info = []\n",
    "    soup = BeautifulSoup(response_content, \"html.parser\")\n",
    "    scripts = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
    "    \n",
    "    for script in scripts:\n",
    "        data = json.loads(script.string)\n",
    "        property_type = data.get(\"@type\", \"\")\n",
    "        street_address = data[\"address\"][\"streetAddress\"] if \"address\" in data else \"\"\n",
    "        locality = data[\"address\"][\"addressLocality\"] if \"address\" in data else \"\"\n",
    "        region = data[\"address\"][\"addressRegion\"] if \"address\" in data else \"\"\n",
    "        postal_code = data[\"address\"][\"postalCode\"] if \"address\" in data else \"\"\n",
    "        floor_size = data[\"floorSize\"][\"value\"] if \"floorSize\" in data else \"\"\n",
    "        latitude = data[\"geo\"][\"latitude\"] if \"geo\" in data else \"\"\n",
    "        longitude = data[\"geo\"][\"longitude\"] if \"geo\" in data else \"\"\n",
    "        url = data.get(\"url\", \"\")\n",
    "        \n",
    "        property_info.append({\n",
    "            \"property_type\": property_type,\n",
    "            \"floor_size\": floor_size,\n",
    "            \"street_address\": street_address,\n",
    "            \"locality\": locality,\n",
    "            \"region\": region,\n",
    "            \"postal_code\": postal_code,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"url\": url\n",
    "        })\n",
    "\n",
    "    return property_info\n",
    "\n",
    "\n",
    "#print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store listing information\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Iterate through each listing card and extract relevant information\n",
    "            # Extract building type and neighborhood\n",
    "        building_info = card.find(class_=\"listingCardLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        # Extract address\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "\n",
    "        # Extract price\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        # Extract bed, bathroom details, and size\n",
    "        description = card.find('div', class_='description').text\n",
    "        bedrooms = card.find('div', class_='bedrooms').text\n",
    "        bathrooms = card.find('div', class_='bathrooms').text\n",
    "        size = card.find('div', class_='size').text\n",
    "\n",
    "        # Extract amenities match\n",
    "        amenities_list = card.find('ul', class_='amenities').find_all('li')\n",
    "        amenities = [amenity.text for amenity in amenities_list]\n",
    "\n",
    "        # property url\n",
    "        url = card.find('a', class_='listingCard-link jsCardLinkGA featured-link-to-hdp')['href']\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"address\": address,\n",
    "             \"building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"size (sq. ft.)\": size,\n",
    "            \"price\": price,\n",
    "            \"# bedrooms\": bedrooms,\n",
    "            \"# bathrooms\": bathrooms,\n",
    "            \"amenities\": amenities,\n",
    "            \"description\": description,\n",
    "            \"url\": url\n",
    "        \n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "        # Append the listing dictionary to the list of listings\n",
    "        \n",
    "            #address = data.find_all(class_= 'list-card-addr') zillow\n",
    "            #price = list(data.find_all(class_='list-card-price')) zillow\n",
    "            #beds = list(data.find_all(\"ul\", class_=\"list-card-details\")) zillow\n",
    "            #last_updated = data.find_all('div', {'class': 'list-card-top'}) zillow\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage:\n",
    "# Define the URL from which you want to fetch the HTML content\n",
    "#url = \"your_url_here\"\n",
    "\n",
    "# Fetch the HTML content from the URL\n",
    "#soup = soups(url)\n",
    "\n",
    "# Parse the HTML content and extract listing information from JSON-LD data\n",
    "listings = parse_soup(soup)\n",
    "\n",
    "# Create a pandas DataFrame from the extracted listing information\n",
    "df = pd.DataFrame(listings)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_zillow_data(url, cache_file):\n",
    "    \"\"\"\n",
    "    Fetches Zillow data from the provided URL, either by fetching it from the cache or making a new request.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL to fetch Zillow data from.\n",
    "    - cache_file (str): The file path for caching the fetched data.\n",
    "\n",
    "    Returns:\n",
    "    - response_content (str): The content of the HTTP response containing Zillow data.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        headers = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-encoding': 'gzip, deflate, br',\n",
    "            'Accept-language': 'en-US,en;q=0.8',\n",
    "            'Upgrade-insecure-requests': '1',\n",
    "            'User-agent': UserAgent().random,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            response_content = response.content\n",
    "            with open(cache_file, 'w') as f:\n",
    "                f.write(response_content.decode('utf-8'))\n",
    "            return response_content\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def extract_property_info(response_content):\n",
    "    property_info = []\n",
    "    \"\"\"\n",
    "    Extracts property information from the response content.\n",
    "\n",
    "    Parameters:\n",
    "    - response_content (str): The content of the HTTP response containing Zillow data.\n",
    "\n",
    "    Returns:\n",
    "    - property_info (list): List of dictionaries containing unique property information.\n",
    "    \n",
    "    if not response_content:\n",
    "        print(\"Error: Response content is empty.\")\n",
    "        return []\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(response_content, \"html.parser\")\n",
    "        zpid_elements = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
    "\n",
    "        #Extracting data from the __NEXT_DATA__ script tag\n",
    "        #next_data_script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "        #if next_data_script:\n",
    "            #try:\n",
    "                #next_data = json.loads(next_data_script.string)\n",
    "                # Prettify the JSON data\n",
    "                #prettified_data = json.dumps(next_data, indent=2)\n",
    "                #print(prettified_data)\n",
    "                # Access and process the data as needed\n",
    "                # Example: data = next_data['props']['pageProps']['your_data_key']\n",
    "            #except json.JSONDecodeError as e:\n",
    "                #print(f\"Error decoding JSON from __NEXT_DATA__: {e}\")\n",
    "\n",
    "        for script in zpid_elements:\n",
    "            data = json.loads(script.string)\n",
    "            url = data.get(\"url\", \"\")\n",
    "            zpid_parts = url.split(\"/\")\n",
    "            if len(zpid_parts) >= 2:\n",
    "                zpid_str = zpid_parts[-2]\n",
    "                zpid_numeric = ''.join(filter(str.isdigit, zpid_str))\n",
    "                zpid = int(zpid_numeric) if zpid_numeric else 0\n",
    "            else:\n",
    "                zpid = 0\n",
    "\n",
    "                property_details = {\n",
    "                \"zpid\": zpid,\n",
    "                \"property_type\": \"\",\n",
    "                \"price\": \"\",\n",
    "                \"street_address\": \"\",\n",
    "                \"locality\": \"\",\n",
    "                \"region\": \"\",\n",
    "                \"postal_code\": \"\",\n",
    "                \"floor_size\": \"\",\n",
    "                \"bedrooms\": \"\",\n",
    "                \"bathrooms\": \"\",\n",
    "                \"sqft\": \"\",\n",
    "                \"days_on_zillow\": \"\",\n",
    "                \"latitude\": \"\",\n",
    "                \"longitude\": \"\",\n",
    "                \"url\": url,\n",
    "            }\n",
    "                \n",
    "            # Extract property details from JSON data\n",
    "            if \"@type\" in data:\n",
    "                address_data = data.get(\"address\", {})\n",
    "                property_details[\"street_address\"] = address_data.get(\"streetAddress\", \"\")\n",
    "                property_details[\"locality\"] = address_data.get(\"addressLocality\", \"\")\n",
    "                property_details[\"region\"] = address_data.get(\"addressRegion\", \"\")\n",
    "                property_details[\"postal_code\"] = address_data.get(\"postalCode\", \"\")\n",
    "\n",
    "            if \"floorSize\" in data and not property_details[\"floor_size\"]:\n",
    "                property_details[\"floor_size\"] = ''.join(filter(str.isdigit, data.get(\"floorSize\", {}).get(\"value\", \"\")))\n",
    "\n",
    "            if \"geo\" in data:\n",
    "                property_details[\"latitude\"] = data[\"geo\"].get(\"latitude\", \"\")\n",
    "                property_details[\"longitude\"] = data[\"geo\"].get(\"longitude\", \"\")\n",
    "\n",
    "            # Additional property details extraction if needed\n",
    "            corresponding_html = soup.find(\"article\", {\"id\": lambda x: x and x.startswith(f\"zpid_{zpid}\")})\n",
    "            if corresponding_html:\n",
    "                if not property_details[\"street_address\"]:\n",
    "                    address_element = corresponding_html.find(\"address\", {\"data-test\": \"property-card-addr\"})\n",
    "                    if address_element:\n",
    "                        address = address_element.get_text(strip=True)\n",
    "                        property_details[\"street_address\"] = address\n",
    "\n",
    "                if not property_details[\"price\"]:\n",
    "                    price_element = corresponding_html.find(\"span\", {\"data-test\": \"property-card-price\"})\n",
    "                    if price_element:\n",
    "                        property_details[\"price\"] = price_element.get_text(strip=True)\n",
    "\n",
    "                ul_elements = corresponding_html.find_all(\"ul\", class_=\"StyledPropertyCardHomeDetailsList-c11n-8-84-3__sc-1xvdaej-0\")\n",
    "                for ul in ul_elements:\n",
    "                    lis = ul.find_all(\"li\")\n",
    "                    for li in lis:\n",
    "                        abbr = li.find(\"abbr\")\n",
    "                        if abbr:\n",
    "                            abbr_text = abbr.text.strip().lower()\n",
    "                            if \"bd\" in abbr_text:\n",
    "                                property_details[\"bedrooms\"] = li.find(\"b\").text.strip()\n",
    "                            elif \"ba\" in abbr_text:\n",
    "                                property_details[\"bathrooms\"] = li.find(\"b\").text.strip()\n",
    "                            elif \"sqft\" in abbr_text:\n",
    "                                property_details[\"sqft\"] = li.find(\"b\").text.strip()                                \n",
    "\n",
    "                # Extracting property types\n",
    "                div_elements = corresponding_html.find_all(\"div\", class_=\"StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 dbDWjx\")\n",
    "                for div in div_elements:\n",
    "                    text_content = div.get_text().strip()            \n",
    "\n",
    "                    # Split the text on \"-\" character and take the first part as the property type\n",
    "                    parts = text_content.split(\"-\")\n",
    "                    if len(parts) > 1:\n",
    "                        property_type = parts[0].strip()\n",
    "                    else:\n",
    "                        # If the text doesn't contain \"-\", use the entire text as the property type\n",
    "                        property_type = text_content.strip()\n",
    "\n",
    "                    # Append the extracted property type to the property_types list\n",
    "                    property_details[\"property_type\"] = property_type\n",
    "\n",
    "\n",
    "                # Extracting days on Zillow\n",
    "                days_on_zillow_element = corresponding_html.find(\"span\", class_=\"StyledPropertyCardBadge-c11n-8-84-3__sc-6gojrl-0\")\n",
    "                if days_on_zillow_element:\n",
    "                    property_details[\"days_on_zillow\"] = days_on_zillow_element.text.strip()\n",
    "\n",
    "\n",
    "                # Validate property details\n",
    "                if property_details[\"zpid\"] != 0 and property_details[\"street_address\"] and property_details[\"price\"]:\n",
    "                    property_info.append(property_details)\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON data: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during extraction: {e}\")\n",
    "\n",
    "    return property_info\n",
    "\n",
    "\n",
    "#    url = f\"{base_url}{page_number}_p/?searchQueryState=%7B%22pagination%22%3A%7B%22currentPage%22%3A{page_number}%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22west%22%3A-74.92725180078128%2C%22east%22%3A-73.03211019921878%2C%22south%22%3A40.27379533285657%2C%22north%22%3A41.11922552941539%7D%2C%22mapZoom%22%3A9%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A6181%2C%22regionType%22%3A6%7D%5D%2C%22filterState%22%3A%7B%22fr%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%7D\"\n",
    "\n",
    "\n",
    "#dfs = []  # List to store DataFrames\n",
    "\n",
    "def main():\n",
    "    rental_base_url = \"https://www.zillow.com/new-york-ny/rentals/\"\n",
    "    sales_base_url = \"https://www.zillow.com/new-york-ny/sales/\"\n",
    "    urls = []\n",
    "\n",
    "    pages_to_fetch = 2  # Adjust the number of pages to fetch\n",
    "\n",
    "    for i in range(1, pages_to_fetch + 1):\n",
    "        if i == 1:\n",
    "           urls.append(rental_base_url) #for rentals\n",
    "            #urls.append(sales_base_url) #for sales\n",
    "        else:\n",
    "            rental_url = f\"{rental_base_url}{i}_p/\"#?searchQueryState=%7B%22pagination%22%3A%7B%22currentPage%22%3A{i}%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22west%22%3A-74.92725180078128%2C%22east%22%3A-73.03211019921878%2C%22south%22%3A40.3962734787787%2C%22north%22%3A40.99807055129214%7D%2C%22mapZoom%22%3A9%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A6181%2C%22regionType%22%3A6%7D%5D%2C%22filterState%22%3A%7B%22fr%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%7D\"\n",
    "            #sales_url = f\"{sales_base_url}{i}_p/\"\n",
    "            urls.append(rental_url)\n",
    "            #urls.append(sales_url)\n",
    "\n",
    "    print(\"Number of URLs to fetch:\", len(urls))\n",
    "    print(\"URLs to fetch:\")\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "\n",
    "    # Iterate through the URLs and fetch Zillow data\n",
    "    for url in urls:\n",
    "        cache_file = f\"zillow_cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
    "        response_content = fetch_zillow_data(url, cache_file)\n",
    "\n",
    "        if response_content:\n",
    "            # Extract property information\n",
    "            property_info = extract_property_info(response_content)\n",
    "\n",
    "            # Display the extracted property information\n",
    "            print(f\"Property Information for {url}:\")\n",
    "            for property_details in property_info:\n",
    "                print(property_details)\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "             # Prettify and save the response content as a .txt file\n",
    "            prettified_content = BeautifulSoup(response_content, \"html.parser\").prettify()\n",
    "            txt_file_name = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_zillow_page.txt\"\n",
    "            with open(txt_file_name, \"w\") as txt_file:\n",
    "                txt_file.write(prettified_content)\n",
    "            print(f\"Prettified HTML content saved as {txt_file_name}\")\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "''' \n",
    "for web_url in urls:\n",
    "    cache_file = \"zillow_cache.html\"\n",
    "    if response_content:\n",
    "        response_content = fetch_zillow_data(web_url, cache_file)\n",
    "        properties = extract_property_info(response_content)\n",
    "        soup = BeautifulSoup(response_content, 'html.parser')\n",
    "        #prettified_data = soup.prettify(response_content)\n",
    "        #print(soup.prettify())\n",
    "\n",
    "\n",
    "\n",
    "#    \n",
    "#        \n",
    "#        df = pd.DataFrame(properties)\n",
    "#        dfs.append(df)  # Append each DataFrame to the list\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "#    if dfs:\n",
    "#        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "#        print(combined_df)\n",
    "#        csv_file_title = f\"{len(combined_df)}_listings_{datetime.now().strftime('%Y-%m-%d_%H-%M')}.csv\"\n",
    "#        combined_df.to_csv(csv_file_title, index=False)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Define a defaultdict to store the occurrence count of each address\n",
    "address_counter = defaultdict(int)\n",
    "\n",
    "def extract_numeric_value(text):\n",
    "    if text is not None:\n",
    "        # Use regular expression to extract numerical values, including optional units\n",
    "        numeric_value = re.search(r'(\\d{1,3}(,\\d{3})*(\\.\\d+)?)\\s*(?:square\\s*feet)?', text)\n",
    "        if numeric_value:\n",
    "            # Remove commas from the extracted value\n",
    "            numeric_value_without_commas = numeric_value.group(1).replace(',', '')\n",
    "            return numeric_value_without_commas  # Return the extracted numeric value\n",
    "    return None\n",
    "\n",
    "# Function to parse the HTML soup\n",
    "def parse_soup(soup):\n",
    "    listings = []\n",
    "\n",
    "    # Find all listing cards on the page\n",
    "    listing_cards = soup.find_all(class_=\"listingCard\")\n",
    "\n",
    "    for card in listing_cards:\n",
    "        # Extract relevant information from each listing card\n",
    "        address = card.find('address', class_='listingCard-addressLabel listingCard-upperShortLabel').get_text().strip()\n",
    "        \n",
    "        building_info = card.find('p',class_=\"listingCardLabel listingCardLabel-grey listingCard-upperShortLabel\")\n",
    "        building_type_neighborhood = building_info.text.strip()\n",
    "\n",
    "        price = card.find('span', class_='price listingCard-priceMargin').get_text().strip()\n",
    "\n",
    "        bed_elem = card.find('span', class_='listingDetailDefinitionsIcon--bed')\n",
    "        beds_text = bed_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bed_elem else None\n",
    "        beds = extract_numeric_value(beds_text)\n",
    "\n",
    "        bath_elem = card.find('span', class_='listingDetailDefinitionsIcon--bath')\n",
    "        baths_text = bath_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if bath_elem else None\n",
    "        baths = extract_numeric_value(baths_text)\n",
    "\n",
    "        # Extract size\n",
    "        size_elem = card.find('span', class_='listingDetailDefinitionsIcon--measure')\n",
    "        size_text = size_elem.find_next_sibling('span', class_='listingDetailDefinitionsText').text.strip() if size_elem else None\n",
    "        size = extract_numeric_value(size_text)\n",
    "\n",
    "        url_element = card.find('a', class_='listingCard-link jsCardLinkGA')\n",
    "        url = url_element.get('href') if url_element else 'Property URL not found.'\n",
    "\n",
    "        # Update the address counter\n",
    "        address_counter[address] += 1\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        extracted_listings = {\n",
    "            \"Address\": address,\n",
    "            \"Building_type_neighborhood\": building_type_neighborhood,\n",
    "            \"Size (sq. ft.)\": size,\n",
    "            \"Price\": price,\n",
    "            \"Bedrooms #\": beds,\n",
    "            \"Bathrooms # \": baths,\n",
    "            \"Url\": url\n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        listings.append(extracted_listings)\n",
    "\n",
    "    return listings\n",
    "\n",
    "# Call the parse_soup function to extract listing information\n",
    "all_listings = []  # List to store all dataframes\n",
    "\n",
    "for url in urls:\n",
    "    soup = soups(url)\n",
    "    listings = parse_soup(soup)\n",
    "    df = pd.DataFrame(listings)\n",
    "    all_listings.append(df)  # Append the dataframe to the list of dataframes\n",
    "\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(all_listings, ignore_index=True)\n",
    "# Print the concatenated DataFrame\n",
    "\n",
    "# Display DataFrame with revised structure and formatting\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# Filter out duplicates and ensure at least one unique occurrence of each address\n",
    "unique_addresses = [address for address, count in address_counter.items() if count == 1]\n",
    "print(f\"Total number of unique addresses: {len(unique_addresses)}\")\n",
    "unique_df = combined_df[combined_df['Address'].isin(unique_addresses)]\n",
    "\n",
    "# Export DataFrame to Excel file with date and page range in the file name\n",
    "date_created = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "excel_file_name = f\"unique_listings_{date_created}_pages_{start_page}_to_{end_page}.xlsx\"\n",
    "#combined_df.to_excel(excel_file_name, index=False)\n",
    "unique_df.to_excel(excel_file_name, index=False)\n",
    "print(f\"Unique DataFrame exported to {excel_file_name}\")\n",
    "print(unique_df)\n",
    "\n",
    "#print(\"Unique DataFrame:\")\n",
    "#print(unique_df)\n",
    "\n",
    "# Print the total number of unique addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os   \n",
    "\n",
    "# Directory containing the HTML files\n",
    "directory = 'Pretty_Resources/Zillow/Zillow Rentals/'\n",
    "\n",
    "# Initialize empty lists or dictionaries to store aggregated data\n",
    "all_address_details = {}\n",
    "all_listing_info = []\n",
    "\n",
    "# Function to parse HTML content from a file\n",
    "def parse_html_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "    return html_content\n",
    "\n",
    "def extract_home_details(html_content):\n",
    "    \"\"\"\n",
    "    Extracts unique home details corresponding to each address from the provided HTML content.\n",
    "\n",
    "    Args:\n",
    "    - html_content (str): The HTML content to parse.\n",
    "\n",
    "    Returns:\n",
    "    - address_details_dict (dict): A dictionary where keys are addresses and values are lists of unique home details.\n",
    "    \"\"\"\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all list items within the main ul\n",
    "    list_items = soup.find_all('li', class_='ListItem-c11n-8-84-3__sc-10e22w8-0 StyledListCardWrapper-srp__sc-wtsrtn-0 iCyebE gTOWtl')\n",
    "    \n",
    "    # Initialize a dictionary to store address-details pairs\n",
    "    address_details_dict = {}\n",
    "    \n",
    "    # Iterate through the list items\n",
    "    for item in list_items:\n",
    "        # Extract address information\n",
    "        address_tag = item.find('a', class_='StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 jnnxAW property-card-link')\n",
    "        if address_tag:\n",
    "            address_info = address_tag.get_text(strip=True)\n",
    "        \n",
    "        # Find the ul containing home details within each list item\n",
    "        home_details_ul = item.find('ul', class_='StyledPropertyCardHomeDetailsList-c11n-8-84-3__sc-1xvdaej-0 eYPFID')\n",
    "        if home_details_ul:\n",
    "            # Extract and print the text content of each li within the ul\n",
    "            home_details = [li.get_text(strip=True) for li in home_details_ul.find_all('li')]\n",
    "            # Add details to the dictionary corresponding to the address\n",
    "            if address_info in address_details_dict:\n",
    "                address_details_dict[address_info].extend(home_details)\n",
    "            else:\n",
    "                address_details_dict[address_info] = home_details\n",
    "\n",
    "    # Remove duplicates from the details lists\n",
    "    for address, details in address_details_dict.items():\n",
    "        address_details_dict[address] = list(set(details))\n",
    "\n",
    "    return address_details_dict\n",
    "\n",
    "\n",
    "def extract_listing_info(html_content):\n",
    "    \"\"\"\n",
    "    Extracts information from various types of script tags containing JSON data.\n",
    "\n",
    "    Args:\n",
    "    - html_content (str): The HTML content to parse.\n",
    "\n",
    "    Returns:\n",
    "    - json_list (list): A list of dictionaries containing information from each JSON content.\n",
    "    \"\"\"\n",
    "    json_list = []\n",
    "\n",
    "    try:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all <script> tags\n",
    "        script_tags = soup.find_all('script')\n",
    "\n",
    "        for script_tag in script_tags:\n",
    "            script_content = script_tag.string\n",
    "\n",
    "            if script_content:\n",
    "                # Check if the script contains JSON data\n",
    "                try:\n",
    "                    json_content = json.loads(script_content)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip if not JSON data\n",
    "\n",
    "                # Extract data based on the script type\n",
    "                if 'name' in json_content and 'startDate' in json_content:\n",
    "                    # This is likely event data\n",
    "                    event_info = {\n",
    "                        'name': json_content.get('name', ''),\n",
    "                    #    'startDate': json_content.get('startDate', ''),\n",
    "                    #    'endDate': json_content.get('endDate', ''),\n",
    "                        'url': json_content.get('url', ''),\n",
    "                    #    'image': json_content.get('image', ''),\n",
    "                    #    'location': json_content.get('location', [])\n",
    "                    }\n",
    "                    json_list.append(event_info)\n",
    "                elif 'props' in json_content and 'pageProps' in json_content['props']:\n",
    "                    # This may contain page structure and search results\n",
    "                    page_props = json_content['props']['pageProps']\n",
    "                    if 'searchResults' in page_props:\n",
    "                        search_results = page_props['searchResults'].get('listResults', [])\n",
    "                        for result in search_results:\n",
    "                            result_info = {\n",
    "                                'zpid': result.get('zpid', ''),\n",
    "                                'property_type': result.get('statusType', ''),\n",
    "                                'address': result.get('address', ''),\n",
    "                                'price': result.get('units', [])[0].get('price', ''),\n",
    "                                'beds': result.get('units', [])[0].get('beds', ''),\n",
    "                                'url': result.get('detailUrl', '')\n",
    "                            }\n",
    "                            json_list.append(result_info)\n",
    "                    elif 'regionState' in page_props:\n",
    "                        # Extract region information\n",
    "                        region_info = page_props.get('regionState', {}).get('regionInfo', [])\n",
    "                        for region in region_info:\n",
    "                            region_info = {\n",
    "                                'regionId': region.get('regionId', ''),\n",
    "                                'regionType': region.get('regionType', ''),\n",
    "                                'regionName': region.get('regionName', '')\n",
    "                            }\n",
    "                            json_list.append(region_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    return json_list\n",
    "\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.html'):  # Process only HTML files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Call the function to parse HTML content\n",
    "        html_content = parse_html_file(file_path)\n",
    "        \n",
    "        # Call the function to extract home details\n",
    "        address_details_dict = extract_home_details(html_content)\n",
    "        \n",
    "        # Call the function to extract listing info\n",
    "        listing_info = extract_listing_info(html_content)\n",
    "        \n",
    "        # Aggregate data into the lists or dictionaries\n",
    "        all_address_details.update(address_details_dict)\n",
    "        all_listing_info.extend(listing_info)\n",
    "\n",
    "# Now, you can process or store the aggregated data as needed\n",
    "print(\"Aggregated Home Details:\", all_address_details)\n",
    "print(\"Aggregated Listing Info:\", all_listing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some listings the information on number of bedroom, number of bathroom, and apartment size is incomplete or mixed up. I performed data manipulation to fix the mistaken values and clean up the extra symbols such as comma and dollar sign. <br\\ >\n",
    "Finally, I have two data sets containing the housing information for apartments for rent and apartments for sale. My for sale data set has 8,456 rows and 8 columns, and the for rent data set has 20,988 rows and 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#is the apartment furnished?\n",
    "cond=data['bed']=='Furnished'\n",
    "data.loc[cond,'furnished']=1\n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from size to bath\n",
    "cond=[]\n",
    "for i in data['size']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'size'] \n",
    "data.loc[cond,'size']=''\n",
    "\n",
    "#move from bed to bath\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='Furnished' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        cond.append(i.split(\" \")[1] in ('bath','baths'))\n",
    "data.loc[cond,'bath']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "#move from bath to bed\n",
    "cond=[]\n",
    "for i in data['bath']:\n",
    "    if(i==''):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(len(i.split(\" \"))==1):\n",
    "            cond.append(True)\n",
    "        else:\n",
    "            if(i.split(\" \")[1] in ('bath','baths')):\n",
    "                cond.append(False)\n",
    "            else:\n",
    "                cond.append(True)\n",
    "data.loc[cond,'bed']=data.loc[cond,'bath'] \n",
    "data.loc[cond,'bath']=''\n",
    "\n",
    "#move from bed to size\n",
    "cond=[]\n",
    "for i in data['bed']:\n",
    "    if(i=='' or i=='studio'):\n",
    "        cond.append(False)\n",
    "    else:\n",
    "        if(i.split(\" \")[1] in ('bed','beds')):\n",
    "            cond.append(False)\n",
    "        else:\n",
    "            cond.append(True)\n",
    "data.loc[cond,'size']=data.loc[cond,'bed'] \n",
    "data.loc[cond,'bed']=''\n",
    "\n",
    "\n",
    "#replace blank with nan\n",
    "data=data.applymap(lambda x: np.nan if x=='' else x)\n",
    "\n",
    "#data\n",
    "data.to_csv('rent.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size to numeric\n",
    "cond=data['size'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'size']=int(data['size'][i].split(\" \")[0].replace(',',''))\n",
    "#bath to numeric\n",
    "cond=data['bath'].isnull()\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bath']=float(data['bath'][i].split(\" \")[0].replace('+',''))\n",
    "#bed to numeric\n",
    "cond=data['bed'].isnull()\n",
    "data['bed']=data['bed'].replace('studio','0 bed')\n",
    "for i in range(0,len(cond)):\n",
    "    if (not cond[i]):\n",
    "        data.loc[i,'bed']=float(data['bed'][i].split(\" \")[0].replace(',','').replace('+',''))\n",
    "#remove dollar sign\n",
    "data['price']=[int(i.replace('$','').replace(',','')) for i in data['price']]\n",
    "\n",
    "data.to_csv('rent_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "book = 'zillow_sales_search_results.txt'\n",
    "#book = 'zillow_rental_search_results.txt'\n",
    "\n",
    "ZPID_Set = set()\n",
    "\n",
    "with open(book, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if '\"zpid\"' in line:  # Ensure that only lines with proper ZPID information are considered\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) >= 2: #This code snippet checks if the length of the list or string variable 'parts' is greater than or equal to 2. \n",
    "                ZPID = parts[1].strip().rstrip(',').replace('\"', '')  # Extracting the value of ZPID from the line\n",
    "                if ZPID not in ZPID_Set:  # Check if ZPID is already in the set\n",
    "                    ZPID_Set.add(ZPID)  # Add ZPID to the set\n",
    "\n",
    "print('Total Number of lines: ', len(lines))\n",
    "print('Total number of unique ZPID: ', len(ZPID_Set))\n",
    "print(ZPID_Set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one below works specifically for parsing local JSON local sales info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def parse_zillow_results_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    try:\n",
    "        zillow_data = json.loads(data)\n",
    "        list_results = zillow_data['searchResults']['listResults']\n",
    "        return list_results\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_zillow_properties(properties):\n",
    "    parsed_properties = []\n",
    "    for property_data in properties:\n",
    "        parsed_property = {\n",
    "            'ID': property_data.get('zpid', ''),\n",
    "            'Property Type': property_data.get('statusText', ''),\n",
    "            'Price': property_data.get('price', ''),\n",
    "            'Address': property_data.get('addressStreet', ''),\n",
    "            'City': property_data.get('addressCity', ''),\n",
    "            'State': property_data.get('addressState', ''),\n",
    "            'Zipcode': property_data.get('addressZipcode', ''),\n",
    "            'Beds': property_data.get('beds', 0),\n",
    "            'Baths': property_data.get('baths', 0),\n",
    "            'Area (sq. ft)': property_data.get('area', 0),\n",
    "            'Zestimate': property_data.get('hdpData', {}).get('homeInfo', {}).get('zestimate', 0),\n",
    "            'RentZestimate': property_data.get('hdpData', {}).get('homeInfo', {}).get('rentZestimate', 0),\n",
    "            'Days on Zillow': property_data.get('hdpData', {}).get('homeInfo', {}).get('daysOnZillow', 0),\n",
    "            'Website': property_data.get('detailUrl',0)\n",
    "        }\n",
    "        parsed_properties.append(parsed_property)\n",
    "    return parsed_properties\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'zillow_sales_search_results.txt'\n",
    "properties = parse_zillow_results_from_file(file_path)\n",
    "parsed_properties = parse_zillow_properties(properties)\n",
    "\n",
    "print('# Properties Parsed: ', len(parsed_properties))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(parsed_properties)\n",
    "\n",
    "# Convert numeric columns to integers\n",
    "numeric_columns = ['Beds', 'Baths', 'Area (sq. ft)', 'Zestimate', 'RentZestimate', 'Days on Zillow']\n",
    "for col in numeric_columns:\n",
    "    df[col] = df[col].astype(int) # Convert to float\n",
    "# Clean and convert 'price' column\n",
    "df['Price'] = df['Price'].str.replace('[\\$,]', '', regex=True)  # Remove '$' and ','\n",
    "df['Price'] = df['Price'].str.replace('K', '000').str.replace('M', '000000')  # Replace 'K' and 'M' with zeros\n",
    "df['Property Type'] = df['Property Type'].str.replace('for sale', '', regex=True)  # Remove '$' and ','\n",
    "\n",
    "# Count the number of unique ZPID values\n",
    "unique_zpids = df['ID'].nunique()\n",
    "print('Number of unique ZPIDs:', unique_zpids)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Convert to numeric with coercion to handle non-numeric values\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert non-numeric values to NaN\n",
    "\n",
    "# Drop non-numeric columns\n",
    "numeric_df = df[numeric_columns]\n",
    "\n",
    "#for col in df.columns:\n",
    "#    unique_values = df[col].unique()\n",
    "#    print(f\"Unique values in {col}: {unique_values}\")\n",
    "\n",
    "\n",
    "# Perform data analysis and visualization here\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "book = 'zillow_rental_search_p1.txt'\n",
    "#book = 'zillow_rental_search_results.txt'\n",
    "\n",
    "ZPID_Set = set()\n",
    "\n",
    "with open(book, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if '\"zpid\"' in line:  # Ensure that only lines with proper ZPID information are considered\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) >= 2: #This code snippet checks if the length of the list or string variable 'parts' is greater than or equal to 2. \n",
    "                ZPID = parts[1].strip().rstrip(',').replace('\"', '')  # Extracting the value of ZPID from the line\n",
    "                if ZPID not in ZPID_Set:  # Check if ZPID is already in the set\n",
    "                    ZPID_Set.add(ZPID)  # Add ZPID to the set\n",
    "\n",
    "print('Total Number of lines: ', len(lines))\n",
    "print('Total number of unique ZPID: ', len(ZPID_Set))\n",
    "print(ZPID_Set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ZPID, Address, Property_Type, Price, Beds, Baths, Sqft, URL, Page]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the HTML files\n",
    "directory = 'Pretty_Resources/Zillow/Zillow Rentals/'\n",
    "\n",
    "# Initialize an empty list to store listing information\n",
    "listing_info_list = []\n",
    "\n",
    "# Function to parse HTML content from a file\n",
    "def parse_html_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "    return html_content\n",
    "\n",
    "# Function to extract home and listing information\n",
    "def extract_home_and_listing_info(html_content, page_name):\n",
    "    \"\"\"\n",
    "    Extracts unique home details and listing information corresponding to each address \n",
    "    from the provided HTML content.\n",
    "\n",
    "    Args:\n",
    "    - html_content (str): The HTML content to parse.\n",
    "    - page_name (str): The name of the page associated with the HTML content.\n",
    "\n",
    "    Returns:\n",
    "    - listing_info_list (list): A list of tuples containing listing information.\n",
    "    \"\"\"\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all list items within the main ul\n",
    "    list_items = soup.find_all('li', class_='ListItem-c11n-8-84-3__sc-10e22w8-0 StyledListCardWrapper-srp__sc-wtsrtn-0 iCyebE gTOWtl')\n",
    "    \n",
    "    # Iterate through the list items\n",
    "    for item in list_items:\n",
    "        # Extract address information\n",
    "        address_tag = item.find('a', class_='StyledPropertyCardDataArea-c11n-8-84-3__sc-yipmu-0 jnnxAW property-card-link')\n",
    "        if address_tag:\n",
    "            address_info = address_tag.get_text(strip=True)\n",
    "        \n",
    "            # Find the ul containing home details within each list item\n",
    "            home_details_ul = item.find('ul', class_='StyledPropertyCardHomeDetailsList-c11n-8-84-3__sc-1xvdaej-0 eYPFID')\n",
    "            if home_details_ul:\n",
    "                # Extract variables to store home details\n",
    "                beds, baths, sqft, price = None, None, None, None\n",
    "                # Extract the text content of each li within the ul\n",
    "                home_details_list = home_details_ul.find_all('li')\n",
    "                for detail in home_details_list:\n",
    "                    text = detail.get_text(strip=True)\n",
    "                    if 'bd' in text:\n",
    "                        beds = text\n",
    "                    elif 'ba' in text:\n",
    "                        baths = text\n",
    "                    elif 'sqft' in text:\n",
    "                        sqft = text\n",
    "                    elif '$' in text:\n",
    "                        price = text\n",
    "\n",
    "                # Find JSON script tags containing listing information\n",
    "                script_tags = item.find_all('script')\n",
    "                for script_tag in script_tags:\n",
    "                    script_content = script_tag.string\n",
    "                    if script_content:\n",
    "                        try:\n",
    "                            json_content = json.loads(script_content)\n",
    "                            if 'props' in json_content and 'pageProps' in json_content['props']:\n",
    "                                page_props = json_content['props']['pageProps']\n",
    "                                if 'searchResults' in page_props:\n",
    "                                    search_results = page_props['searchResults'].get('listResults', [])\n",
    "                                    for result in search_results:\n",
    "                                        listing_info = {\n",
    "                                            'address': address_info,\n",
    "                                            'zpid': result.get('zpid', ''),\n",
    "                                            'property_type': result.get('statusType', ''),\n",
    "                                            'price': result.get('units', [])[0].get('price', ''),\n",
    "                                            'beds': result.get('units', [])[0].get('beds', ''),\n",
    "                                            'baths': result.get('units', [])[0].get('baths', ''),\n",
    "                                            'sqft': result.get('units', [])[0].get('area', ''),\n",
    "                                            'url': result.get('detailUrl', ''),\n",
    "                                            'page': page_name\n",
    "                                        }\n",
    "                                        listing_info_list.append(listing_info)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "    \n",
    "    return listing_info_list\n",
    "\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.html'):  # Process only HTML files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        page_name = os.path.splitext(filename)[0]  # Get the page name without the extension\n",
    "        \n",
    "        # Call the function to parse HTML content\n",
    "        html_content = parse_html_file(file_path)\n",
    "        \n",
    "        # Call the function to extract home details and listing info\n",
    "        extract_home_and_listing_info(html_content, page_name)\n",
    "        \n",
    "# Convert the list of tuples to a Pandas DataFrame\n",
    "columns = ['ZPID', 'Address', 'Property_Type', 'Price', 'Beds', 'Baths', 'Sqft', 'URL', 'Page']\n",
    "df = pd.DataFrame(listing_info_list, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch_page_content(url_or_filepath):\n",
    "    # Check if the URL is a local file path\n",
    "    if os.path.exists(url_or_filepath):\n",
    "        # Open the local file and read its content\n",
    "        with open(url_or_filepath, 'r') as file:\n",
    "            content = file.read()\n",
    "    else:\n",
    "        # Fetch the page content using requests\n",
    "        response = requests.get(url_or_filepath)\n",
    "        content = response.content\n",
    "    return content\n",
    "\n",
    "def extract_rental_info(rental_element):\n",
    "    rental = {}\n",
    "\n",
    "    # Extract ZPID for each rental element\n",
    "    rental['zpid'] = rental_element.find('article')['id'].split('_')[-1]\n",
    "\n",
    "    # Extract Event Information\n",
    "    event_script = rental_element.find('script', type='application/ld+json')\n",
    "    if event_script:\n",
    "        event_data = json.loads(event_script.text)\n",
    "        event_location = event_data.get('location', [{}])[1].get('address', {})\n",
    "\n",
    "        rental.update({\n",
    "            'event_image': event_data.get('image', ''),\n",
    "            'event_start_date': event_data.get('startDate', ''),\n",
    "            'event_end_date': event_data.get('endDate', ''),\n",
    "            'event_street_address': event_location.get('streetAddress', ''),\n",
    "            'event_postal_code': event_location.get('postalCode', ''),\n",
    "            'event_address_locality': event_location.get('addressLocality', ''),\n",
    "            'event_address_region': event_location.get('addressRegion', '')\n",
    "        })\n",
    "\n",
    "    # Extract Property Details\n",
    "    property_address_element = rental_element.find('address', {'data-test': 'property-card-addr'})\n",
    "    rental['property_address'] = property_address_element.text.strip() if property_address_element else ''\n",
    "\n",
    "    # Extract Prices\n",
    "    price_elements = rental_element.find_all('span', {'class': 'PropertyCardWrapper__StyledPriceLine-srp__sc-16e8gqd-1.iMKTKr'})\n",
    "    rental['prices'] = [elem.text.strip() for elem in price_elements] if price_elements else []\n",
    "\n",
    "    # Extract Property Web URL\n",
    "    property_web_url_element = rental_element.find('a', {'data-test': 'property-card-link'})\n",
    "    rental['property_web_url'] = property_web_url_element['href'] if property_web_url_element else ''\n",
    "\n",
    "    return rental\n",
    "\n",
    "def parse_rentals_page(url_or_filepath):\n",
    "    content = fetch_page_content(url_or_filepath)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    rentals = []\n",
    "    rental_elements = soup.find_all('div', {'data-renderstrat': 'ssr'})\n",
    "\n",
    "    for rental_element in rental_elements:\n",
    "        rental_info = extract_rental_info(rental_element)\n",
    "        rentals.append(rental_info)\n",
    "\n",
    "    return rentals\n",
    "\n",
    "def extract_sales_info(sales_element):\n",
    "    sale = {}\n",
    "\n",
    "    # Extract ZPID for each sales element\n",
    "    sale['zpid'] = sales_element.find('article')['id'].split('_')[-1]\n",
    "\n",
    "    # Extract Property Details\n",
    "    property_address_element = sales_element.find('address', {'data-address-label': 'property-address'})\n",
    "    sale['property_address'] = property_address_element.text.strip() if property_address_element else ''\n",
    "\n",
    "    # Extract Prices\n",
    "    price_element = sales_element.find('span', {'class': 'Text-c11n-8-84-3__aiai24-0 iZVatD'})\n",
    "    sale['price'] = price_element.text.strip() if price_element else ''\n",
    "\n",
    "    # Extract Property Web URL\n",
    "    property_web_url_element = sales_element.find('a', {'class': 'Anchor-c11n-8-84-3__sc-hn4bge-0 sc-ckEbSK kxrUt bnQEvw'})\n",
    "    sale['property_web_url'] = property_web_url_element['href'] if property_web_url_element else ''\n",
    "\n",
    "    return sale\n",
    "\n",
    "def parse_sales_page(url_or_filepath):\n",
    "    content = fetch_page_content(url_or_filepath)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    sales = []\n",
    "    sales_elements = soup.find_all('div', {'data-testid': 'home-card-sale'})\n",
    "\n",
    "    for sales_element in sales_elements:\n",
    "        sale_info = extract_sales_info(sales_element)\n",
    "        sales.append(sale_info)\n",
    "\n",
    "    return sales\n",
    "\n",
    "def fetch_and_parse_data(base_url, parse_function, pages_to_fetch, file_path=None):\n",
    "    all_data = []\n",
    "    for page_number in range(1, pages_to_fetch + 1):\n",
    "        if file_path:\n",
    "            url_or_filepath = file_path\n",
    "        else:\n",
    "            url_or_filepath = f\"{base_url}{page_number}_p/\"\n",
    "        data = parse_function(url_or_filepath)\n",
    "        all_data.extend(data)\n",
    "    return all_data\n",
    "\n",
    "def parse_pagination_number(url):\n",
    "    content = fetch_page_content(url)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    pagination_elements = soup.find_all('a', {'class': 'StyledPaginationButton-c11n-8-84-3__aiai24-1'})\n",
    "    if pagination_elements:\n",
    "        last_page_number = max(int(page.text) for page in pagination_elements if page.text.isdigit())\n",
    "        return last_page_number\n",
    "    return 0  \n",
    "\n",
    "def main():\n",
    "    # Choose between online source or local file\n",
    "    source_choice = input(\"Enter 'online' to use a website or 'local file' to use a file: \").lower()\n",
    "    if source_choice not in ['online', 'local file']:\n",
    "        print(\"Invalid choice.\")\n",
    "        return\n",
    "\n",
    "    file_path = None  # Initialize file_path outside of the if block\n",
    "\n",
    "    if source_choice == 'online':\n",
    "        # Choose between rentals or sales\n",
    "        choice = input(\"Enter 'rentals' or 'sales' to fetch data: \").lower()\n",
    "        if choice not in ['rentals', 'sales']:\n",
    "            print(\"Invalid choice.\")\n",
    "            return\n",
    "        # Fetch the URL based on the choice\n",
    "        if choice == 'rentals':\n",
    "            base_url = \"https://www.zillow.com/new-york-ny/rentals/\"\n",
    "        elif choice == 'sales':\n",
    "            base_url = \"https://www.zillow.com/new-york-ny/sales/\"\n",
    "        else:\n",
    "            print(\"Invalid choice.\")\n",
    "            return\n",
    "        # Get the number of pages to fetch\n",
    "        pages_to_fetch = parse_pagination_number(base_url)\n",
    "    else:  # Source choice is local file\n",
    "        file_path = input(\"Enter the local file path: \")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"File not found.\")\n",
    "            return\n",
    "        # No need for pagination logic for local files\n",
    "        pages_to_fetch = 1\n",
    "\n",
    "    # Prompt for the pages to scrape\n",
    "    pages_input = input(\"Enter 'all' to scrape all pages, 'range' to specify a range, or 'specific' for a specific page: \")\n",
    "    if pages_input.lower() == 'all':\n",
    "        pages_to_scrape = 'all'\n",
    "    elif pages_input.lower() == 'range':\n",
    "        start_page = int(input(\"Enter the start page: \"))\n",
    "        end_page = int(input(\"Enter the end page: \"))\n",
    "        pages_to_scrape = (start_page, end_page)\n",
    "    elif pages_input.lower() == 'specific':\n",
    "        specific_page = int(input(\"Enter the specific page: \"))\n",
    "        pages_to_scrape = (specific_page,)\n",
    "    else:\n",
    "        print(\"Invalid input for pages.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Pages to Scrape:\", pages_to_scrape)\n",
    "\n",
    "\n",
    "    # Fetch and parse the data\n",
    "    if choice == 'rentals':\n",
    "        parse_function = parse_rentals_page\n",
    "    elif choice == 'sales':\n",
    "        parse_function = parse_sales_page\n",
    "\n",
    "    all_data = fetch_and_parse_data(base_url, parse_function, pages_to_fetch, file_path)\n",
    "    all_data_df = pd.DataFrame(all_data)\n",
    "    if all_data_df.empty:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "    unique_zpids_count = all_data_df['zpid'].nunique()\n",
    "    \n",
    "\n",
    "    print(f\"\\n{choice.capitalize()} DataFrame:\")\n",
    "    print(all_data_df)\n",
    "    print(f\"\\nNumber of Unique {choice.capitalize()} ZPIDs:\", unique_zpids_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
